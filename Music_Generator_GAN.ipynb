{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Generator - GAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaymathur777/AI-Project/blob/master/Music_Generator_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeYhWIN9L6i5",
        "colab_type": "code",
        "outputId": "4aad3625-13dd-4365-959c-20b758212118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY-AkTbWRs4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    with open('/content/drive/My Drive/AIAssignment/data/notes (1)', 'rb') as filepath:\n",
        "      notes = pickle.load(filepath)\n",
        "      n_vocab = len(set(notes))\n",
        "      network_input, network_output = prepare_sequences(notes, n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqF_GN_Lc9t",
        "colab_type": "code",
        "outputId": "24d7cffb-a5fc-4bcf-a895-3d96205ce4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def get_notes():\n",
        "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(\"/content/drive/My Drive/AIAssignment/midi_songs/*.mid\"):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try: # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse() \n",
        "        except: # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "            \n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return notes\n",
        "\n",
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # Create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    \n",
        "    # Normalize input between -1 and 1\n",
        "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)\n",
        "\n",
        "def generate_notes(model, network_input, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "    \n",
        "    # Get pitch names and store in a dictionary\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "        \n",
        "        pattern = numpy.append(pattern,index)\n",
        "        #pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output\n",
        "  \n",
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item[0]\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self, rows):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 1000\n",
        "        self.disc_loss = []\n",
        "        self.gen_loss =[]\n",
        "        \n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates note sequences\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        generated_seq = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(generated_seq)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
        "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        seq = Input(shape=self.seq_shape)\n",
        "        validity = model(seq)\n",
        "\n",
        "        return Model(seq, validity)\n",
        "      \n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.seq_shape))\n",
        "        model.summary()\n",
        "        \n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        seq = model(noise)\n",
        "\n",
        "        return Model(noise, seq)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load and convert the data\n",
        "        notes = get_notes()\n",
        "        n_vocab = len(set(notes))\n",
        "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        \n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Training the discriminator\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_seqs = X_train[idx]\n",
        "\n",
        "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
        "            #noise = (noise-242)/242\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new note sequences\n",
        "            gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "            #  Training the Generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, real)\n",
        "\n",
        "            # Print the progress and save into loss lists\n",
        "            if epoch % sample_interval == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "              self.disc_loss.append(d_loss[0])\n",
        "              self.gen_loss.append(g_loss)\n",
        "        \n",
        "        self.generate(notes)\n",
        "        self.plot_loss()\n",
        "        \n",
        "    def generate(self, input_notes):\n",
        "        # Get pitch names and store in a dictionary\n",
        "        notes = input_notes\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "        \n",
        "        # Use random noise to generate sequences\n",
        "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "        predictions = self.generator.predict(noise)\n",
        "        \n",
        "        pred_notes = [((x+1)*(n_vocab-1)/2) for x in predictions[0]]\n",
        "        print(type(pred_notes))\n",
        "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
        "        \n",
        "        create_midi(pred_notes, 'gan_final')\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss, c='red')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gan = GAN(rows=100)    \n",
        "  gan.train(epochs=1000, batch_size=32, sample_interval=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_7 (CuDNNLSTM)     (None, 100, 512)          1054720   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 1024)              4202496   \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,913,601\n",
            "Trainable params: 5,913,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 256)               256256    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               102500    \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 100, 1)            0         \n",
            "=================================================================\n",
            "Total params: 1,022,820\n",
            "Trainable params: 1,019,236\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/0fithos.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/8.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ahead_on_our_way_piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/AT.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/balamb.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/bcm.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/BlueStone_LastDungeon.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/braska.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Cids.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/caitsith.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/cosmo.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/dayafter.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/decisive.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/DOS.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/costadsol.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/dontbeafraid.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/electric_de_chocobo.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Eternal_Harvest.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/EyesOnMePiano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff11_awakening_piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FF3_Battle_(Piano).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff1battp.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff4-airship.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FF3_Third_Phase_Final_(Piano).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Ff4-BattleLust.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff4-town.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff4-fight1.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FF4.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff4_piano_collections-main_theme.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff4pclov.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FF6epitaph_piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff7-mainmidi.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Ff7-Cinco.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Ff7-One_Winged.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff6shap.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Ff7-Jenova_Absolute.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff7themep.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ff8-lfp.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFIII_Edgar_And_Sabin_Piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FF8_Shuffle_or_boogie_pc.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFIXQuMarshP (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Fierce_Battle_(Piano) (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFIX_Piano (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Fiend_Battle_(Piano) (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFVII_BATTLE (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/figaro (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Finalfantasy6fanfarecomplete (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Final_Fantasy_7_-_Judgement_Day_Piano (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Finalfantasy5gilgameshp (1).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFIXQuMarshP.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFVII_BATTLE.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFIX_Piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Fiend_Battle_(Piano).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Fierce_Battle_(Piano).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Finalfantasy6fanfarecomplete.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Finalfantasy5gilgameshp.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/figaro.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Final_Fantasy_Matouyas_Cave_Piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/fortresscondor.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Fyw_piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/gerudo.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/goldsaucer.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/great_war.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Gold_Silver_Rival_Battle.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/HighwindTakestotheSkies.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/In_Zanarkand.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/JENOVA.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Kingdom_Hearts_Dearly_Beloved.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Life_Stream.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/path_of_repentance.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Rachel_Piano_tempofix.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Kingdom_Hearts_Traverse_Town.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/pkelite4.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/lurk_in_dark.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/OTD5YA.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/mining.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Oppressed.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/redwings.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/relmstheme-piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/roseofmay-piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/rufus.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Rydia_pc.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/sandy.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Still_Alive-1.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Suteki_Da_Ne_(Piano_Version).mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/sera_.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/sobf.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/thenightmarebegins.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/tpirtsd-piano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ultimafro.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/traitor.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/tifap.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/thoughts.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/VincentPiano.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ultros.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/ViviinAlexandria.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/waltz_de_choco.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/Zelda_Overworld.mid\n",
            "Parsing /content/drive/My Drive/AIAssignment/midi_songs/z_aeristhemepiano.mid\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.699120, acc.: 1.56%] [G loss: 0.690375]\n",
            "1 [D loss: 0.678967, acc.: 64.06%] [G loss: 0.690938]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2 [D loss: 0.647781, acc.: 78.12%] [G loss: 0.694615]\n",
            "3 [D loss: 0.603416, acc.: 70.31%] [G loss: 0.711154]\n",
            "4 [D loss: 0.578147, acc.: 65.62%] [G loss: 0.820908]\n",
            "5 [D loss: 0.393977, acc.: 89.06%] [G loss: 1.318074]\n",
            "6 [D loss: 0.537578, acc.: 87.50%] [G loss: 1.329504]\n",
            "7 [D loss: 0.341086, acc.: 89.06%] [G loss: 1.715653]\n",
            "8 [D loss: 0.335558, acc.: 87.50%] [G loss: 3.242121]\n",
            "9 [D loss: 0.334772, acc.: 89.06%] [G loss: 1.631843]\n",
            "10 [D loss: 0.273697, acc.: 89.06%] [G loss: 2.057350]\n",
            "11 [D loss: 0.284750, acc.: 90.62%] [G loss: 2.738435]\n",
            "12 [D loss: 0.222784, acc.: 90.62%] [G loss: 2.433073]\n",
            "13 [D loss: 0.379418, acc.: 87.50%] [G loss: 1.809575]\n",
            "14 [D loss: 0.258129, acc.: 92.19%] [G loss: 2.010545]\n",
            "15 [D loss: 0.290856, acc.: 93.75%] [G loss: 2.164557]\n",
            "16 [D loss: 0.417298, acc.: 89.06%] [G loss: 1.746155]\n",
            "17 [D loss: 0.396102, acc.: 82.81%] [G loss: 1.979462]\n",
            "18 [D loss: 0.382216, acc.: 85.94%] [G loss: 1.859935]\n",
            "19 [D loss: 0.382724, acc.: 85.94%] [G loss: 1.866174]\n",
            "20 [D loss: 0.373368, acc.: 84.38%] [G loss: 1.794593]\n",
            "21 [D loss: 0.358398, acc.: 87.50%] [G loss: 1.871935]\n",
            "22 [D loss: 0.304844, acc.: 89.06%] [G loss: 2.424268]\n",
            "23 [D loss: 0.282556, acc.: 90.62%] [G loss: 2.451741]\n",
            "24 [D loss: 0.395329, acc.: 84.38%] [G loss: 1.745596]\n",
            "25 [D loss: 0.302248, acc.: 90.62%] [G loss: 1.947934]\n",
            "26 [D loss: 0.474941, acc.: 76.56%] [G loss: 1.654190]\n",
            "27 [D loss: 0.347705, acc.: 85.94%] [G loss: 1.933453]\n",
            "28 [D loss: 0.310948, acc.: 87.50%] [G loss: 2.082255]\n",
            "29 [D loss: 0.608934, acc.: 73.44%] [G loss: 1.391243]\n",
            "30 [D loss: 0.436603, acc.: 82.81%] [G loss: 1.390420]\n",
            "31 [D loss: 0.438302, acc.: 82.81%] [G loss: 1.476738]\n",
            "32 [D loss: 0.390583, acc.: 81.25%] [G loss: 1.611886]\n",
            "33 [D loss: 0.470604, acc.: 76.56%] [G loss: 1.601866]\n",
            "34 [D loss: 0.509241, acc.: 78.12%] [G loss: 1.396538]\n",
            "35 [D loss: 0.438472, acc.: 79.69%] [G loss: 1.411991]\n",
            "36 [D loss: 0.331938, acc.: 85.94%] [G loss: 1.755222]\n",
            "37 [D loss: 0.319380, acc.: 85.94%] [G loss: 1.807947]\n",
            "38 [D loss: 0.532675, acc.: 76.56%] [G loss: 1.441714]\n",
            "39 [D loss: 0.353381, acc.: 84.38%] [G loss: 1.521551]\n",
            "40 [D loss: 0.322290, acc.: 87.50%] [G loss: 1.811821]\n",
            "41 [D loss: 0.496227, acc.: 75.00%] [G loss: 1.763617]\n",
            "42 [D loss: 0.418405, acc.: 85.94%] [G loss: 1.750217]\n",
            "43 [D loss: 0.348295, acc.: 87.50%] [G loss: 1.782743]\n",
            "44 [D loss: 0.337454, acc.: 84.38%] [G loss: 1.901688]\n",
            "45 [D loss: 0.264919, acc.: 87.50%] [G loss: 2.322187]\n",
            "46 [D loss: 0.403003, acc.: 85.94%] [G loss: 2.000326]\n",
            "47 [D loss: 0.456143, acc.: 87.50%] [G loss: 1.581764]\n",
            "48 [D loss: 0.402429, acc.: 87.50%] [G loss: 1.625805]\n",
            "49 [D loss: 0.419400, acc.: 76.56%] [G loss: 1.780290]\n",
            "50 [D loss: 0.340263, acc.: 92.19%] [G loss: 2.155926]\n",
            "51 [D loss: 0.487392, acc.: 85.94%] [G loss: 1.642110]\n",
            "52 [D loss: 0.584502, acc.: 76.56%] [G loss: 1.412633]\n",
            "53 [D loss: 0.388437, acc.: 84.38%] [G loss: 1.485423]\n",
            "54 [D loss: 0.534540, acc.: 79.69%] [G loss: 1.396439]\n",
            "55 [D loss: 0.399249, acc.: 84.38%] [G loss: 1.380654]\n",
            "56 [D loss: 0.442207, acc.: 82.81%] [G loss: 1.398771]\n",
            "57 [D loss: 0.473351, acc.: 78.12%] [G loss: 1.383344]\n",
            "58 [D loss: 0.461890, acc.: 79.69%] [G loss: 1.429554]\n",
            "59 [D loss: 0.391758, acc.: 84.38%] [G loss: 1.458986]\n",
            "60 [D loss: 0.336506, acc.: 84.38%] [G loss: 1.810730]\n",
            "61 [D loss: 0.330799, acc.: 85.94%] [G loss: 1.877516]\n",
            "62 [D loss: 0.553703, acc.: 79.69%] [G loss: 1.326000]\n",
            "63 [D loss: 0.545168, acc.: 71.88%] [G loss: 1.332101]\n",
            "64 [D loss: 0.428332, acc.: 87.50%] [G loss: 1.355488]\n",
            "65 [D loss: 0.573641, acc.: 71.88%] [G loss: 1.283676]\n",
            "66 [D loss: 0.491427, acc.: 79.69%] [G loss: 1.319616]\n",
            "67 [D loss: 0.442908, acc.: 85.94%] [G loss: 1.435063]\n",
            "68 [D loss: 0.412184, acc.: 79.69%] [G loss: 1.519154]\n",
            "69 [D loss: 0.601341, acc.: 71.88%] [G loss: 1.467205]\n",
            "70 [D loss: 0.492555, acc.: 78.12%] [G loss: 1.467656]\n",
            "71 [D loss: 0.392748, acc.: 85.94%] [G loss: 1.729165]\n",
            "72 [D loss: 0.385148, acc.: 85.94%] [G loss: 2.078952]\n",
            "73 [D loss: 0.557105, acc.: 71.88%] [G loss: 1.699741]\n",
            "74 [D loss: 0.349558, acc.: 85.94%] [G loss: 1.702097]\n",
            "75 [D loss: 0.505455, acc.: 79.69%] [G loss: 1.830688]\n",
            "76 [D loss: 0.426134, acc.: 79.69%] [G loss: 2.078624]\n",
            "77 [D loss: 0.669616, acc.: 71.88%] [G loss: 1.360023]\n",
            "78 [D loss: 0.452942, acc.: 81.25%] [G loss: 1.362629]\n",
            "79 [D loss: 0.395547, acc.: 82.81%] [G loss: 1.545467]\n",
            "80 [D loss: 0.509620, acc.: 79.69%] [G loss: 1.830368]\n",
            "81 [D loss: 0.408077, acc.: 85.94%] [G loss: 1.652833]\n",
            "82 [D loss: 0.427093, acc.: 78.12%] [G loss: 2.062536]\n",
            "83 [D loss: 0.464620, acc.: 73.44%] [G loss: 2.192394]\n",
            "84 [D loss: 0.407437, acc.: 82.81%] [G loss: 2.055139]\n",
            "85 [D loss: 0.361419, acc.: 84.38%] [G loss: 1.840587]\n",
            "86 [D loss: 0.541444, acc.: 75.00%] [G loss: 1.468782]\n",
            "87 [D loss: 0.537799, acc.: 75.00%] [G loss: 1.230061]\n",
            "88 [D loss: 0.483069, acc.: 79.69%] [G loss: 1.310840]\n",
            "89 [D loss: 0.425449, acc.: 79.69%] [G loss: 1.443353]\n",
            "90 [D loss: 0.536319, acc.: 73.44%] [G loss: 1.439434]\n",
            "91 [D loss: 0.596337, acc.: 65.62%] [G loss: 1.183947]\n",
            "92 [D loss: 0.507139, acc.: 76.56%] [G loss: 1.241099]\n",
            "93 [D loss: 0.560656, acc.: 75.00%] [G loss: 1.349942]\n",
            "94 [D loss: 0.550402, acc.: 70.31%] [G loss: 1.486328]\n",
            "95 [D loss: 0.522696, acc.: 75.00%] [G loss: 1.660862]\n",
            "96 [D loss: 0.469251, acc.: 76.56%] [G loss: 2.066098]\n",
            "97 [D loss: 0.605137, acc.: 75.00%] [G loss: 1.151402]\n",
            "98 [D loss: 0.580739, acc.: 73.44%] [G loss: 1.164962]\n",
            "99 [D loss: 0.498107, acc.: 75.00%] [G loss: 1.335672]\n",
            "100 [D loss: 0.549330, acc.: 73.44%] [G loss: 1.488330]\n",
            "101 [D loss: 0.526968, acc.: 75.00%] [G loss: 1.483501]\n",
            "102 [D loss: 0.399832, acc.: 82.81%] [G loss: 1.586954]\n",
            "103 [D loss: 0.540008, acc.: 75.00%] [G loss: 1.518045]\n",
            "104 [D loss: 0.494598, acc.: 73.44%] [G loss: 1.589118]\n",
            "105 [D loss: 0.563562, acc.: 78.12%] [G loss: 1.334962]\n",
            "106 [D loss: 0.602717, acc.: 73.44%] [G loss: 1.276564]\n",
            "107 [D loss: 0.659632, acc.: 59.38%] [G loss: 1.153440]\n",
            "108 [D loss: 0.647349, acc.: 65.62%] [G loss: 1.016330]\n",
            "109 [D loss: 0.551624, acc.: 73.44%] [G loss: 1.115176]\n",
            "110 [D loss: 0.607273, acc.: 67.19%] [G loss: 1.128706]\n",
            "111 [D loss: 0.537933, acc.: 70.31%] [G loss: 1.258238]\n",
            "112 [D loss: 0.512415, acc.: 71.88%] [G loss: 1.397235]\n",
            "113 [D loss: 0.712997, acc.: 60.94%] [G loss: 1.054551]\n",
            "114 [D loss: 0.540829, acc.: 70.31%] [G loss: 1.049142]\n",
            "115 [D loss: 0.610138, acc.: 62.50%] [G loss: 1.092547]\n",
            "116 [D loss: 0.547714, acc.: 78.12%] [G loss: 1.159219]\n",
            "117 [D loss: 0.591023, acc.: 67.19%] [G loss: 1.207121]\n",
            "118 [D loss: 0.538057, acc.: 73.44%] [G loss: 1.219944]\n",
            "119 [D loss: 0.568955, acc.: 68.75%] [G loss: 1.199960]\n",
            "120 [D loss: 0.583102, acc.: 71.88%] [G loss: 1.254303]\n",
            "121 [D loss: 0.485566, acc.: 79.69%] [G loss: 1.341953]\n",
            "122 [D loss: 0.459703, acc.: 79.69%] [G loss: 1.426180]\n",
            "123 [D loss: 0.614507, acc.: 68.75%] [G loss: 1.167680]\n",
            "124 [D loss: 0.489270, acc.: 76.56%] [G loss: 1.329435]\n",
            "125 [D loss: 0.670663, acc.: 64.06%] [G loss: 1.102848]\n",
            "126 [D loss: 0.555566, acc.: 75.00%] [G loss: 1.167577]\n",
            "127 [D loss: 0.611091, acc.: 65.62%] [G loss: 1.149765]\n",
            "128 [D loss: 0.495241, acc.: 76.56%] [G loss: 1.430162]\n",
            "129 [D loss: 0.459530, acc.: 79.69%] [G loss: 1.713769]\n",
            "130 [D loss: 0.612355, acc.: 64.06%] [G loss: 1.203667]\n",
            "131 [D loss: 0.466185, acc.: 79.69%] [G loss: 1.294606]\n",
            "132 [D loss: 0.709909, acc.: 62.50%] [G loss: 1.074285]\n",
            "133 [D loss: 0.528127, acc.: 73.44%] [G loss: 1.192108]\n",
            "134 [D loss: 0.639251, acc.: 59.38%] [G loss: 1.055057]\n",
            "135 [D loss: 0.597625, acc.: 65.62%] [G loss: 1.096846]\n",
            "136 [D loss: 0.564764, acc.: 71.88%] [G loss: 1.307135]\n",
            "137 [D loss: 0.672289, acc.: 67.19%] [G loss: 1.030119]\n",
            "138 [D loss: 0.623795, acc.: 65.62%] [G loss: 1.051965]\n",
            "139 [D loss: 0.730726, acc.: 51.56%] [G loss: 0.942543]\n",
            "140 [D loss: 0.554626, acc.: 76.56%] [G loss: 1.027725]\n",
            "141 [D loss: 0.694351, acc.: 60.94%] [G loss: 1.097770]\n",
            "142 [D loss: 0.618790, acc.: 62.50%] [G loss: 1.049172]\n",
            "143 [D loss: 0.592740, acc.: 68.75%] [G loss: 1.107113]\n",
            "144 [D loss: 0.571355, acc.: 73.44%] [G loss: 1.200484]\n",
            "145 [D loss: 0.602489, acc.: 67.19%] [G loss: 1.300372]\n",
            "146 [D loss: 0.592615, acc.: 64.06%] [G loss: 1.142565]\n",
            "147 [D loss: 0.584083, acc.: 68.75%] [G loss: 1.172647]\n",
            "148 [D loss: 0.596445, acc.: 71.88%] [G loss: 1.130526]\n",
            "149 [D loss: 0.564535, acc.: 71.88%] [G loss: 1.268752]\n",
            "150 [D loss: 0.613060, acc.: 71.88%] [G loss: 1.092173]\n",
            "151 [D loss: 0.642170, acc.: 64.06%] [G loss: 1.130721]\n",
            "152 [D loss: 0.609061, acc.: 59.38%] [G loss: 1.205951]\n",
            "153 [D loss: 0.560836, acc.: 71.88%] [G loss: 1.296216]\n",
            "154 [D loss: 0.655467, acc.: 68.75%] [G loss: 1.142350]\n",
            "155 [D loss: 0.617474, acc.: 67.19%] [G loss: 1.126124]\n",
            "156 [D loss: 0.460402, acc.: 79.69%] [G loss: 2.202261]\n",
            "157 [D loss: 0.592539, acc.: 76.56%] [G loss: 1.173977]\n",
            "158 [D loss: 0.529854, acc.: 70.31%] [G loss: 1.159594]\n",
            "159 [D loss: 0.499073, acc.: 76.56%] [G loss: 1.475008]\n",
            "160 [D loss: 0.561486, acc.: 75.00%] [G loss: 1.545417]\n",
            "161 [D loss: 0.424299, acc.: 84.38%] [G loss: 1.848571]\n",
            "162 [D loss: 0.664148, acc.: 71.88%] [G loss: 1.334104]\n",
            "163 [D loss: 0.621245, acc.: 67.19%] [G loss: 1.082123]\n",
            "164 [D loss: 0.607175, acc.: 62.50%] [G loss: 1.097929]\n",
            "165 [D loss: 0.643084, acc.: 60.94%] [G loss: 1.171427]\n",
            "166 [D loss: 0.536971, acc.: 81.25%] [G loss: 1.340680]\n",
            "167 [D loss: 0.608051, acc.: 71.88%] [G loss: 1.223979]\n",
            "168 [D loss: 0.592285, acc.: 65.62%] [G loss: 1.353629]\n",
            "169 [D loss: 0.720381, acc.: 64.06%] [G loss: 0.881407]\n",
            "170 [D loss: 0.550852, acc.: 81.25%] [G loss: 1.000492]\n",
            "171 [D loss: 0.550169, acc.: 76.56%] [G loss: 1.310127]\n",
            "172 [D loss: 0.471102, acc.: 81.25%] [G loss: 1.521572]\n",
            "173 [D loss: 0.677738, acc.: 62.50%] [G loss: 1.051125]\n",
            "174 [D loss: 0.600482, acc.: 64.06%] [G loss: 0.989180]\n",
            "175 [D loss: 0.606244, acc.: 67.19%] [G loss: 1.031851]\n",
            "176 [D loss: 0.563551, acc.: 68.75%] [G loss: 1.110872]\n",
            "177 [D loss: 0.668983, acc.: 64.06%] [G loss: 1.030422]\n",
            "178 [D loss: 0.743055, acc.: 46.88%] [G loss: 0.885707]\n",
            "179 [D loss: 0.652590, acc.: 57.81%] [G loss: 0.878117]\n",
            "180 [D loss: 0.630823, acc.: 64.06%] [G loss: 0.905402]\n",
            "181 [D loss: 0.609471, acc.: 68.75%] [G loss: 0.957034]\n",
            "182 [D loss: 0.645494, acc.: 60.94%] [G loss: 0.972673]\n",
            "183 [D loss: 0.548664, acc.: 73.44%] [G loss: 1.128175]\n",
            "184 [D loss: 0.600037, acc.: 64.06%] [G loss: 1.119115]\n",
            "185 [D loss: 0.583948, acc.: 65.62%] [G loss: 1.027378]\n",
            "186 [D loss: 0.526640, acc.: 70.31%] [G loss: 1.190655]\n",
            "187 [D loss: 0.551535, acc.: 70.31%] [G loss: 1.184473]\n",
            "188 [D loss: 0.654936, acc.: 62.50%] [G loss: 0.922248]\n",
            "189 [D loss: 0.655125, acc.: 57.81%] [G loss: 0.894176]\n",
            "190 [D loss: 0.601913, acc.: 68.75%] [G loss: 0.919069]\n",
            "191 [D loss: 0.621969, acc.: 67.19%] [G loss: 0.958562]\n",
            "192 [D loss: 0.558324, acc.: 73.44%] [G loss: 1.041399]\n",
            "193 [D loss: 0.469165, acc.: 79.69%] [G loss: 1.238183]\n",
            "194 [D loss: 0.659296, acc.: 68.75%] [G loss: 0.966484]\n",
            "195 [D loss: 0.611074, acc.: 67.19%] [G loss: 0.957943]\n",
            "196 [D loss: 0.626999, acc.: 57.81%] [G loss: 0.929382]\n",
            "197 [D loss: 0.621795, acc.: 62.50%] [G loss: 0.984828]\n",
            "198 [D loss: 0.629044, acc.: 59.38%] [G loss: 0.934958]\n",
            "199 [D loss: 0.669655, acc.: 59.38%] [G loss: 0.926022]\n",
            "200 [D loss: 0.568825, acc.: 68.75%] [G loss: 1.020593]\n",
            "201 [D loss: 0.658441, acc.: 60.94%] [G loss: 0.981491]\n",
            "202 [D loss: 0.651549, acc.: 59.38%] [G loss: 0.919120]\n",
            "203 [D loss: 0.648625, acc.: 62.50%] [G loss: 0.882826]\n",
            "204 [D loss: 0.587121, acc.: 71.88%] [G loss: 0.967136]\n",
            "205 [D loss: 0.664454, acc.: 53.12%] [G loss: 0.944522]\n",
            "206 [D loss: 0.605712, acc.: 75.00%] [G loss: 0.980054]\n",
            "207 [D loss: 0.716254, acc.: 57.81%] [G loss: 0.811504]\n",
            "208 [D loss: 0.685225, acc.: 54.69%] [G loss: 0.796223]\n",
            "209 [D loss: 0.634705, acc.: 59.38%] [G loss: 0.808981]\n",
            "210 [D loss: 0.695547, acc.: 54.69%] [G loss: 0.798925]\n",
            "211 [D loss: 0.651514, acc.: 67.19%] [G loss: 0.824060]\n",
            "212 [D loss: 0.597907, acc.: 60.94%] [G loss: 0.851200]\n",
            "213 [D loss: 0.681874, acc.: 59.38%] [G loss: 0.841930]\n",
            "214 [D loss: 0.649793, acc.: 57.81%] [G loss: 0.833934]\n",
            "215 [D loss: 0.667848, acc.: 59.38%] [G loss: 0.834591]\n",
            "216 [D loss: 0.618345, acc.: 67.19%] [G loss: 0.853673]\n",
            "217 [D loss: 0.708050, acc.: 54.69%] [G loss: 0.842607]\n",
            "218 [D loss: 0.665476, acc.: 62.50%] [G loss: 0.832180]\n",
            "219 [D loss: 0.658253, acc.: 67.19%] [G loss: 0.843709]\n",
            "220 [D loss: 0.684759, acc.: 59.38%] [G loss: 0.822813]\n",
            "221 [D loss: 0.684317, acc.: 56.25%] [G loss: 0.814634]\n",
            "222 [D loss: 0.726722, acc.: 51.56%] [G loss: 0.760669]\n",
            "223 [D loss: 0.667939, acc.: 60.94%] [G loss: 0.775010]\n",
            "224 [D loss: 0.653535, acc.: 56.25%] [G loss: 0.781392]\n",
            "225 [D loss: 0.698302, acc.: 56.25%] [G loss: 0.785423]\n",
            "226 [D loss: 0.716126, acc.: 53.12%] [G loss: 0.749263]\n",
            "227 [D loss: 0.660907, acc.: 56.25%] [G loss: 0.757874]\n",
            "228 [D loss: 0.679917, acc.: 60.94%] [G loss: 0.765028]\n",
            "229 [D loss: 0.696082, acc.: 53.12%] [G loss: 0.777165]\n",
            "230 [D loss: 0.685548, acc.: 50.00%] [G loss: 0.768589]\n",
            "231 [D loss: 0.688858, acc.: 54.69%] [G loss: 0.761779]\n",
            "232 [D loss: 0.635759, acc.: 65.62%] [G loss: 0.787860]\n",
            "233 [D loss: 0.658457, acc.: 60.94%] [G loss: 0.798501]\n",
            "234 [D loss: 0.611820, acc.: 68.75%] [G loss: 0.862902]\n",
            "235 [D loss: 0.705659, acc.: 51.56%] [G loss: 0.846277]\n",
            "236 [D loss: 0.638872, acc.: 60.94%] [G loss: 0.883362]\n",
            "237 [D loss: 0.655152, acc.: 57.81%] [G loss: 0.858350]\n",
            "238 [D loss: 0.699706, acc.: 50.00%] [G loss: 0.818588]\n",
            "239 [D loss: 0.671804, acc.: 62.50%] [G loss: 0.798416]\n",
            "240 [D loss: 0.667483, acc.: 57.81%] [G loss: 0.796683]\n",
            "241 [D loss: 0.700025, acc.: 46.88%] [G loss: 0.818236]\n",
            "242 [D loss: 0.689472, acc.: 56.25%] [G loss: 0.744411]\n",
            "243 [D loss: 0.720105, acc.: 50.00%] [G loss: 0.730101]\n",
            "244 [D loss: 0.686978, acc.: 59.38%] [G loss: 0.736654]\n",
            "245 [D loss: 0.696607, acc.: 51.56%] [G loss: 0.727138]\n",
            "246 [D loss: 0.695740, acc.: 53.12%] [G loss: 0.723431]\n",
            "247 [D loss: 0.675877, acc.: 56.25%] [G loss: 0.731084]\n",
            "248 [D loss: 0.696403, acc.: 51.56%] [G loss: 0.719885]\n",
            "249 [D loss: 0.685533, acc.: 59.38%] [G loss: 0.720646]\n",
            "250 [D loss: 0.706886, acc.: 48.44%] [G loss: 0.708087]\n",
            "251 [D loss: 0.695174, acc.: 53.12%] [G loss: 0.715817]\n",
            "252 [D loss: 0.687402, acc.: 51.56%] [G loss: 0.717724]\n",
            "253 [D loss: 0.689331, acc.: 50.00%] [G loss: 0.713268]\n",
            "254 [D loss: 0.683097, acc.: 59.38%] [G loss: 0.723253]\n",
            "255 [D loss: 0.698369, acc.: 48.44%] [G loss: 0.722377]\n",
            "256 [D loss: 0.681219, acc.: 48.44%] [G loss: 0.715731]\n",
            "257 [D loss: 0.674551, acc.: 60.94%] [G loss: 0.728329]\n",
            "258 [D loss: 0.710311, acc.: 46.88%] [G loss: 0.718557]\n",
            "259 [D loss: 0.689335, acc.: 57.81%] [G loss: 0.711927]\n",
            "260 [D loss: 0.683463, acc.: 59.38%] [G loss: 0.718066]\n",
            "261 [D loss: 0.669666, acc.: 57.81%] [G loss: 0.726675]\n",
            "262 [D loss: 0.638334, acc.: 71.88%] [G loss: 0.762407]\n",
            "263 [D loss: 0.671495, acc.: 60.94%] [G loss: 0.769456]\n",
            "264 [D loss: 0.736075, acc.: 39.06%] [G loss: 0.718903]\n",
            "265 [D loss: 0.683799, acc.: 51.56%] [G loss: 0.722866]\n",
            "266 [D loss: 0.708349, acc.: 42.19%] [G loss: 0.716864]\n",
            "267 [D loss: 0.710076, acc.: 46.88%] [G loss: 0.711495]\n",
            "268 [D loss: 0.697014, acc.: 48.44%] [G loss: 0.706068]\n",
            "269 [D loss: 0.692350, acc.: 57.81%] [G loss: 0.712703]\n",
            "270 [D loss: 0.683606, acc.: 51.56%] [G loss: 0.710844]\n",
            "271 [D loss: 0.705706, acc.: 48.44%] [G loss: 0.706767]\n",
            "272 [D loss: 0.675864, acc.: 60.94%] [G loss: 0.703788]\n",
            "273 [D loss: 0.695686, acc.: 46.88%] [G loss: 0.690657]\n",
            "274 [D loss: 0.697331, acc.: 45.31%] [G loss: 0.710972]\n",
            "275 [D loss: 0.708446, acc.: 50.00%] [G loss: 0.693783]\n",
            "276 [D loss: 0.682440, acc.: 51.56%] [G loss: 0.699080]\n",
            "277 [D loss: 0.696666, acc.: 54.69%] [G loss: 0.691108]\n",
            "278 [D loss: 0.692284, acc.: 53.12%] [G loss: 0.693134]\n",
            "279 [D loss: 0.678377, acc.: 57.81%] [G loss: 0.687191]\n",
            "280 [D loss: 0.694882, acc.: 53.12%] [G loss: 0.685444]\n",
            "281 [D loss: 0.681399, acc.: 62.50%] [G loss: 0.690522]\n",
            "282 [D loss: 0.681212, acc.: 57.81%] [G loss: 0.688960]\n",
            "283 [D loss: 0.683964, acc.: 60.94%] [G loss: 0.693608]\n",
            "284 [D loss: 0.719693, acc.: 43.75%] [G loss: 0.695176]\n",
            "285 [D loss: 0.679020, acc.: 54.69%] [G loss: 0.690566]\n",
            "286 [D loss: 0.689545, acc.: 54.69%] [G loss: 0.689486]\n",
            "287 [D loss: 0.682230, acc.: 51.56%] [G loss: 0.693880]\n",
            "288 [D loss: 0.714267, acc.: 46.88%] [G loss: 0.698200]\n",
            "289 [D loss: 0.701174, acc.: 43.75%] [G loss: 0.698550]\n",
            "290 [D loss: 0.669619, acc.: 59.38%] [G loss: 0.703071]\n",
            "291 [D loss: 0.688049, acc.: 51.56%] [G loss: 0.704655]\n",
            "292 [D loss: 0.693784, acc.: 54.69%] [G loss: 0.703539]\n",
            "293 [D loss: 0.717614, acc.: 40.62%] [G loss: 0.704205]\n",
            "294 [D loss: 0.692041, acc.: 51.56%] [G loss: 0.705409]\n",
            "295 [D loss: 0.696410, acc.: 43.75%] [G loss: 0.701066]\n",
            "296 [D loss: 0.682904, acc.: 53.12%] [G loss: 0.710917]\n",
            "297 [D loss: 0.689480, acc.: 51.56%] [G loss: 0.701482]\n",
            "298 [D loss: 0.689354, acc.: 56.25%] [G loss: 0.697080]\n",
            "299 [D loss: 0.687486, acc.: 53.12%] [G loss: 0.715327]\n",
            "300 [D loss: 0.696118, acc.: 50.00%] [G loss: 0.715289]\n",
            "301 [D loss: 0.671136, acc.: 60.94%] [G loss: 0.714365]\n",
            "302 [D loss: 0.692143, acc.: 45.31%] [G loss: 0.715287]\n",
            "303 [D loss: 0.664848, acc.: 56.25%] [G loss: 0.716243]\n",
            "304 [D loss: 0.690913, acc.: 50.00%] [G loss: 0.717132]\n",
            "305 [D loss: 0.691996, acc.: 56.25%] [G loss: 0.719374]\n",
            "306 [D loss: 0.698991, acc.: 50.00%] [G loss: 0.711284]\n",
            "307 [D loss: 0.661834, acc.: 62.50%] [G loss: 0.717262]\n",
            "308 [D loss: 0.716991, acc.: 43.75%] [G loss: 0.700745]\n",
            "309 [D loss: 0.658822, acc.: 65.62%] [G loss: 0.706750]\n",
            "310 [D loss: 0.685445, acc.: 57.81%] [G loss: 0.716709]\n",
            "311 [D loss: 0.693385, acc.: 48.44%] [G loss: 0.720446]\n",
            "312 [D loss: 0.726299, acc.: 35.94%] [G loss: 0.699203]\n",
            "313 [D loss: 0.703668, acc.: 37.50%] [G loss: 0.711024]\n",
            "314 [D loss: 0.696928, acc.: 50.00%] [G loss: 0.712450]\n",
            "315 [D loss: 0.704968, acc.: 46.88%] [G loss: 0.710686]\n",
            "316 [D loss: 0.691164, acc.: 42.19%] [G loss: 0.707165]\n",
            "317 [D loss: 0.697768, acc.: 48.44%] [G loss: 0.701381]\n",
            "318 [D loss: 0.692101, acc.: 54.69%] [G loss: 0.695383]\n",
            "319 [D loss: 0.685235, acc.: 59.38%] [G loss: 0.704384]\n",
            "320 [D loss: 0.688253, acc.: 53.12%] [G loss: 0.695760]\n",
            "321 [D loss: 0.689328, acc.: 53.12%] [G loss: 0.706798]\n",
            "322 [D loss: 0.694715, acc.: 48.44%] [G loss: 0.702975]\n",
            "323 [D loss: 0.706492, acc.: 46.88%] [G loss: 0.698395]\n",
            "324 [D loss: 0.695210, acc.: 50.00%] [G loss: 0.699903]\n",
            "325 [D loss: 0.685605, acc.: 53.12%] [G loss: 0.700472]\n",
            "326 [D loss: 0.680214, acc.: 59.38%] [G loss: 0.700783]\n",
            "327 [D loss: 0.698360, acc.: 46.88%] [G loss: 0.701136]\n",
            "328 [D loss: 0.696560, acc.: 54.69%] [G loss: 0.702012]\n",
            "329 [D loss: 0.703726, acc.: 43.75%] [G loss: 0.711641]\n",
            "330 [D loss: 0.703830, acc.: 32.81%] [G loss: 0.713004]\n",
            "331 [D loss: 0.694806, acc.: 48.44%] [G loss: 0.711315]\n",
            "332 [D loss: 0.687474, acc.: 56.25%] [G loss: 0.711490]\n",
            "333 [D loss: 0.697672, acc.: 39.06%] [G loss: 0.716048]\n",
            "334 [D loss: 0.704387, acc.: 46.88%] [G loss: 0.711178]\n",
            "335 [D loss: 0.689915, acc.: 59.38%] [G loss: 0.709000]\n",
            "336 [D loss: 0.688187, acc.: 54.69%] [G loss: 0.706943]\n",
            "337 [D loss: 0.685775, acc.: 50.00%] [G loss: 0.714488]\n",
            "338 [D loss: 0.690587, acc.: 53.12%] [G loss: 0.724584]\n",
            "339 [D loss: 0.709030, acc.: 35.94%] [G loss: 0.702684]\n",
            "340 [D loss: 0.688463, acc.: 60.94%] [G loss: 0.700339]\n",
            "341 [D loss: 0.687161, acc.: 53.12%] [G loss: 0.699034]\n",
            "342 [D loss: 0.698736, acc.: 48.44%] [G loss: 0.706691]\n",
            "343 [D loss: 0.699375, acc.: 43.75%] [G loss: 0.703516]\n",
            "344 [D loss: 0.693362, acc.: 46.88%] [G loss: 0.706320]\n",
            "345 [D loss: 0.689829, acc.: 45.31%] [G loss: 0.705387]\n",
            "346 [D loss: 0.694926, acc.: 46.88%] [G loss: 0.703567]\n",
            "347 [D loss: 0.697278, acc.: 43.75%] [G loss: 0.705225]\n",
            "348 [D loss: 0.691685, acc.: 48.44%] [G loss: 0.710998]\n",
            "349 [D loss: 0.696519, acc.: 39.06%] [G loss: 0.704492]\n",
            "350 [D loss: 0.691420, acc.: 53.12%] [G loss: 0.707571]\n",
            "351 [D loss: 0.702473, acc.: 50.00%] [G loss: 0.698869]\n",
            "352 [D loss: 0.697774, acc.: 46.88%] [G loss: 0.692652]\n",
            "353 [D loss: 0.699520, acc.: 40.62%] [G loss: 0.695646]\n",
            "354 [D loss: 0.693203, acc.: 50.00%] [G loss: 0.690178]\n",
            "355 [D loss: 0.698388, acc.: 39.06%] [G loss: 0.696209]\n",
            "356 [D loss: 0.697143, acc.: 46.88%] [G loss: 0.700524]\n",
            "357 [D loss: 0.686125, acc.: 56.25%] [G loss: 0.693091]\n",
            "358 [D loss: 0.704643, acc.: 42.19%] [G loss: 0.704833]\n",
            "359 [D loss: 0.695162, acc.: 48.44%] [G loss: 0.696833]\n",
            "360 [D loss: 0.695416, acc.: 51.56%] [G loss: 0.700134]\n",
            "361 [D loss: 0.702501, acc.: 54.69%] [G loss: 0.693774]\n",
            "362 [D loss: 0.696326, acc.: 43.75%] [G loss: 0.689871]\n",
            "363 [D loss: 0.689917, acc.: 50.00%] [G loss: 0.696132]\n",
            "364 [D loss: 0.696769, acc.: 54.69%] [G loss: 0.688551]\n",
            "365 [D loss: 0.698452, acc.: 50.00%] [G loss: 0.683287]\n",
            "366 [D loss: 0.712679, acc.: 50.00%] [G loss: 0.688779]\n",
            "367 [D loss: 0.709827, acc.: 48.44%] [G loss: 0.702589]\n",
            "368 [D loss: 0.695408, acc.: 45.31%] [G loss: 0.704743]\n",
            "369 [D loss: 0.699029, acc.: 42.19%] [G loss: 0.704479]\n",
            "370 [D loss: 0.695190, acc.: 46.88%] [G loss: 0.704049]\n",
            "371 [D loss: 0.693619, acc.: 39.06%] [G loss: 0.698627]\n",
            "372 [D loss: 0.699701, acc.: 40.62%] [G loss: 0.705758]\n",
            "373 [D loss: 0.697291, acc.: 37.50%] [G loss: 0.706572]\n",
            "374 [D loss: 0.695412, acc.: 39.06%] [G loss: 0.701340]\n",
            "375 [D loss: 0.694544, acc.: 45.31%] [G loss: 0.705120]\n",
            "376 [D loss: 0.695109, acc.: 48.44%] [G loss: 0.704471]\n",
            "377 [D loss: 0.693816, acc.: 51.56%] [G loss: 0.704611]\n",
            "378 [D loss: 0.698507, acc.: 35.94%] [G loss: 0.706048]\n",
            "379 [D loss: 0.694936, acc.: 46.88%] [G loss: 0.708471]\n",
            "380 [D loss: 0.696223, acc.: 45.31%] [G loss: 0.708507]\n",
            "381 [D loss: 0.694120, acc.: 51.56%] [G loss: 0.706950]\n",
            "382 [D loss: 0.692783, acc.: 39.06%] [G loss: 0.707757]\n",
            "383 [D loss: 0.693016, acc.: 53.12%] [G loss: 0.710180]\n",
            "384 [D loss: 0.699067, acc.: 45.31%] [G loss: 0.711068]\n",
            "385 [D loss: 0.691485, acc.: 48.44%] [G loss: 0.708023]\n",
            "386 [D loss: 0.698910, acc.: 51.56%] [G loss: 0.708578]\n",
            "387 [D loss: 0.691683, acc.: 51.56%] [G loss: 0.712804]\n",
            "388 [D loss: 0.691777, acc.: 46.88%] [G loss: 0.708305]\n",
            "389 [D loss: 0.694348, acc.: 40.62%] [G loss: 0.708925]\n",
            "390 [D loss: 0.695047, acc.: 51.56%] [G loss: 0.710678]\n",
            "391 [D loss: 0.691308, acc.: 51.56%] [G loss: 0.710381]\n",
            "392 [D loss: 0.695387, acc.: 48.44%] [G loss: 0.711693]\n",
            "393 [D loss: 0.694609, acc.: 53.12%] [G loss: 0.711612]\n",
            "394 [D loss: 0.693887, acc.: 51.56%] [G loss: 0.708469]\n",
            "395 [D loss: 0.691366, acc.: 51.56%] [G loss: 0.709136]\n",
            "396 [D loss: 0.690624, acc.: 56.25%] [G loss: 0.710892]\n",
            "397 [D loss: 0.694893, acc.: 51.56%] [G loss: 0.709341]\n",
            "398 [D loss: 0.691386, acc.: 54.69%] [G loss: 0.711192]\n",
            "399 [D loss: 0.691997, acc.: 60.94%] [G loss: 0.715584]\n",
            "400 [D loss: 0.696341, acc.: 45.31%] [G loss: 0.712287]\n",
            "401 [D loss: 0.693237, acc.: 50.00%] [G loss: 0.709428]\n",
            "402 [D loss: 0.695769, acc.: 54.69%] [G loss: 0.708536]\n",
            "403 [D loss: 0.692898, acc.: 50.00%] [G loss: 0.706313]\n",
            "404 [D loss: 0.693334, acc.: 53.12%] [G loss: 0.704376]\n",
            "405 [D loss: 0.692537, acc.: 51.56%] [G loss: 0.704659]\n",
            "406 [D loss: 0.692995, acc.: 50.00%] [G loss: 0.710289]\n",
            "407 [D loss: 0.689806, acc.: 51.56%] [G loss: 0.702074]\n",
            "408 [D loss: 0.693485, acc.: 43.75%] [G loss: 0.701505]\n",
            "409 [D loss: 0.692785, acc.: 53.12%] [G loss: 0.708770]\n",
            "410 [D loss: 0.689886, acc.: 54.69%] [G loss: 0.698720]\n",
            "411 [D loss: 0.694006, acc.: 57.81%] [G loss: 0.704892]\n",
            "412 [D loss: 0.693250, acc.: 51.56%] [G loss: 0.706945]\n",
            "413 [D loss: 0.690357, acc.: 48.44%] [G loss: 0.706024]\n",
            "414 [D loss: 0.697513, acc.: 48.44%] [G loss: 0.706723]\n",
            "415 [D loss: 0.684227, acc.: 65.62%] [G loss: 0.702767]\n",
            "416 [D loss: 0.693390, acc.: 50.00%] [G loss: 0.704404]\n",
            "417 [D loss: 0.692781, acc.: 48.44%] [G loss: 0.701000]\n",
            "418 [D loss: 0.692876, acc.: 51.56%] [G loss: 0.698737]\n",
            "419 [D loss: 0.696036, acc.: 50.00%] [G loss: 0.697424]\n",
            "420 [D loss: 0.699548, acc.: 51.56%] [G loss: 0.695316]\n",
            "421 [D loss: 0.694750, acc.: 46.88%] [G loss: 0.695891]\n",
            "422 [D loss: 0.690462, acc.: 56.25%] [G loss: 0.693097]\n",
            "423 [D loss: 0.688546, acc.: 68.75%] [G loss: 0.702578]\n",
            "424 [D loss: 0.694235, acc.: 51.56%] [G loss: 0.698542]\n",
            "425 [D loss: 0.684732, acc.: 57.81%] [G loss: 0.702860]\n",
            "426 [D loss: 0.698547, acc.: 50.00%] [G loss: 0.699124]\n",
            "427 [D loss: 0.701021, acc.: 40.62%] [G loss: 0.692636]\n",
            "428 [D loss: 0.692319, acc.: 57.81%] [G loss: 0.694751]\n",
            "429 [D loss: 0.696271, acc.: 51.56%] [G loss: 0.693455]\n",
            "430 [D loss: 0.699766, acc.: 39.06%] [G loss: 0.691282]\n",
            "431 [D loss: 0.692866, acc.: 43.75%] [G loss: 0.696131]\n",
            "432 [D loss: 0.695092, acc.: 51.56%] [G loss: 0.702322]\n",
            "433 [D loss: 0.693988, acc.: 45.31%] [G loss: 0.693522]\n",
            "434 [D loss: 0.689783, acc.: 56.25%] [G loss: 0.694097]\n",
            "435 [D loss: 0.694067, acc.: 46.88%] [G loss: 0.694452]\n",
            "436 [D loss: 0.699645, acc.: 32.81%] [G loss: 0.696502]\n",
            "437 [D loss: 0.690306, acc.: 53.12%] [G loss: 0.697167]\n",
            "438 [D loss: 0.696071, acc.: 53.12%] [G loss: 0.698681]\n",
            "439 [D loss: 0.698002, acc.: 42.19%] [G loss: 0.698764]\n",
            "440 [D loss: 0.690939, acc.: 53.12%] [G loss: 0.701300]\n",
            "441 [D loss: 0.692055, acc.: 57.81%] [G loss: 0.695623]\n",
            "442 [D loss: 0.690356, acc.: 50.00%] [G loss: 0.694661]\n",
            "443 [D loss: 0.695252, acc.: 51.56%] [G loss: 0.697321]\n",
            "444 [D loss: 0.698983, acc.: 39.06%] [G loss: 0.702713]\n",
            "445 [D loss: 0.694185, acc.: 45.31%] [G loss: 0.704991]\n",
            "446 [D loss: 0.691802, acc.: 50.00%] [G loss: 0.706112]\n",
            "447 [D loss: 0.694615, acc.: 43.75%] [G loss: 0.705287]\n",
            "448 [D loss: 0.692016, acc.: 48.44%] [G loss: 0.707439]\n",
            "449 [D loss: 0.692829, acc.: 42.19%] [G loss: 0.706436]\n",
            "450 [D loss: 0.693243, acc.: 45.31%] [G loss: 0.699962]\n",
            "451 [D loss: 0.690435, acc.: 53.12%] [G loss: 0.704962]\n",
            "452 [D loss: 0.688547, acc.: 56.25%] [G loss: 0.708344]\n",
            "453 [D loss: 0.688030, acc.: 54.69%] [G loss: 0.711249]\n",
            "454 [D loss: 0.695634, acc.: 48.44%] [G loss: 0.709355]\n",
            "455 [D loss: 0.689662, acc.: 56.25%] [G loss: 0.708253]\n",
            "456 [D loss: 0.693440, acc.: 51.56%] [G loss: 0.710186]\n",
            "457 [D loss: 0.694682, acc.: 50.00%] [G loss: 0.707594]\n",
            "458 [D loss: 0.700110, acc.: 39.06%] [G loss: 0.703075]\n",
            "459 [D loss: 0.692189, acc.: 59.38%] [G loss: 0.696827]\n",
            "460 [D loss: 0.692576, acc.: 46.88%] [G loss: 0.694686]\n",
            "461 [D loss: 0.698079, acc.: 51.56%] [G loss: 0.696296]\n",
            "462 [D loss: 0.691566, acc.: 51.56%] [G loss: 0.694972]\n",
            "463 [D loss: 0.697564, acc.: 43.75%] [G loss: 0.696267]\n",
            "464 [D loss: 0.692809, acc.: 57.81%] [G loss: 0.694430]\n",
            "465 [D loss: 0.696495, acc.: 53.12%] [G loss: 0.702505]\n",
            "466 [D loss: 0.692047, acc.: 62.50%] [G loss: 0.696632]\n",
            "467 [D loss: 0.701282, acc.: 34.38%] [G loss: 0.692675]\n",
            "468 [D loss: 0.700118, acc.: 39.06%] [G loss: 0.696499]\n",
            "469 [D loss: 0.692761, acc.: 54.69%] [G loss: 0.695819]\n",
            "470 [D loss: 0.694302, acc.: 50.00%] [G loss: 0.698118]\n",
            "471 [D loss: 0.695715, acc.: 40.62%] [G loss: 0.700366]\n",
            "472 [D loss: 0.694022, acc.: 43.75%] [G loss: 0.696723]\n",
            "473 [D loss: 0.695144, acc.: 51.56%] [G loss: 0.694693]\n",
            "474 [D loss: 0.694531, acc.: 51.56%] [G loss: 0.697327]\n",
            "475 [D loss: 0.691163, acc.: 59.38%] [G loss: 0.701063]\n",
            "476 [D loss: 0.691327, acc.: 57.81%] [G loss: 0.703586]\n",
            "477 [D loss: 0.694101, acc.: 46.88%] [G loss: 0.699721]\n",
            "478 [D loss: 0.695142, acc.: 51.56%] [G loss: 0.700547]\n",
            "479 [D loss: 0.694269, acc.: 45.31%] [G loss: 0.704580]\n",
            "480 [D loss: 0.691155, acc.: 56.25%] [G loss: 0.702582]\n",
            "481 [D loss: 0.693698, acc.: 51.56%] [G loss: 0.702062]\n",
            "482 [D loss: 0.691367, acc.: 56.25%] [G loss: 0.702122]\n",
            "483 [D loss: 0.694486, acc.: 42.19%] [G loss: 0.701508]\n",
            "484 [D loss: 0.692619, acc.: 48.44%] [G loss: 0.700455]\n",
            "485 [D loss: 0.695180, acc.: 42.19%] [G loss: 0.704167]\n",
            "486 [D loss: 0.692443, acc.: 50.00%] [G loss: 0.702658]\n",
            "487 [D loss: 0.692675, acc.: 46.88%] [G loss: 0.703227]\n",
            "488 [D loss: 0.688955, acc.: 54.69%] [G loss: 0.706570]\n",
            "489 [D loss: 0.690642, acc.: 56.25%] [G loss: 0.707553]\n",
            "490 [D loss: 0.697343, acc.: 45.31%] [G loss: 0.704462]\n",
            "491 [D loss: 0.700414, acc.: 39.06%] [G loss: 0.701726]\n",
            "492 [D loss: 0.691068, acc.: 57.81%] [G loss: 0.700858]\n",
            "493 [D loss: 0.693689, acc.: 46.88%] [G loss: 0.697458]\n",
            "494 [D loss: 0.692940, acc.: 53.12%] [G loss: 0.695674]\n",
            "495 [D loss: 0.693135, acc.: 51.56%] [G loss: 0.702020]\n",
            "496 [D loss: 0.691844, acc.: 54.69%] [G loss: 0.702861]\n",
            "497 [D loss: 0.695908, acc.: 45.31%] [G loss: 0.703148]\n",
            "498 [D loss: 0.690966, acc.: 57.81%] [G loss: 0.707277]\n",
            "499 [D loss: 0.693725, acc.: 43.75%] [G loss: 0.703040]\n",
            "500 [D loss: 0.693922, acc.: 46.88%] [G loss: 0.703750]\n",
            "501 [D loss: 0.691541, acc.: 56.25%] [G loss: 0.703668]\n",
            "502 [D loss: 0.696275, acc.: 48.44%] [G loss: 0.702866]\n",
            "503 [D loss: 0.693171, acc.: 43.75%] [G loss: 0.703721]\n",
            "504 [D loss: 0.693295, acc.: 56.25%] [G loss: 0.702461]\n",
            "505 [D loss: 0.695044, acc.: 43.75%] [G loss: 0.704153]\n",
            "506 [D loss: 0.693429, acc.: 54.69%] [G loss: 0.701131]\n",
            "507 [D loss: 0.691141, acc.: 45.31%] [G loss: 0.702601]\n",
            "508 [D loss: 0.698199, acc.: 43.75%] [G loss: 0.702565]\n",
            "509 [D loss: 0.697592, acc.: 40.62%] [G loss: 0.701014]\n",
            "510 [D loss: 0.691548, acc.: 60.94%] [G loss: 0.702528]\n",
            "511 [D loss: 0.695117, acc.: 43.75%] [G loss: 0.704282]\n",
            "512 [D loss: 0.696601, acc.: 39.06%] [G loss: 0.703198]\n",
            "513 [D loss: 0.692213, acc.: 45.31%] [G loss: 0.705645]\n",
            "514 [D loss: 0.694369, acc.: 48.44%] [G loss: 0.703486]\n",
            "515 [D loss: 0.694925, acc.: 50.00%] [G loss: 0.704433]\n",
            "516 [D loss: 0.694393, acc.: 59.38%] [G loss: 0.706751]\n",
            "517 [D loss: 0.694915, acc.: 45.31%] [G loss: 0.701264]\n",
            "518 [D loss: 0.694559, acc.: 48.44%] [G loss: 0.699956]\n",
            "519 [D loss: 0.693318, acc.: 53.12%] [G loss: 0.702993]\n",
            "520 [D loss: 0.690575, acc.: 62.50%] [G loss: 0.706759]\n",
            "521 [D loss: 0.691583, acc.: 62.50%] [G loss: 0.709843]\n",
            "522 [D loss: 0.690414, acc.: 53.12%] [G loss: 0.706285]\n",
            "523 [D loss: 0.691217, acc.: 53.12%] [G loss: 0.710741]\n",
            "524 [D loss: 0.688635, acc.: 60.94%] [G loss: 0.714325]\n",
            "525 [D loss: 0.696505, acc.: 43.75%] [G loss: 0.714506]\n",
            "526 [D loss: 0.692819, acc.: 51.56%] [G loss: 0.712845]\n",
            "527 [D loss: 0.693320, acc.: 46.88%] [G loss: 0.709823]\n",
            "528 [D loss: 0.694018, acc.: 57.81%] [G loss: 0.708844]\n",
            "529 [D loss: 0.690706, acc.: 50.00%] [G loss: 0.709761]\n",
            "530 [D loss: 0.692780, acc.: 54.69%] [G loss: 0.711068]\n",
            "531 [D loss: 0.694819, acc.: 54.69%] [G loss: 0.708562]\n",
            "532 [D loss: 0.693887, acc.: 57.81%] [G loss: 0.707023]\n",
            "533 [D loss: 0.696091, acc.: 46.88%] [G loss: 0.709237]\n",
            "534 [D loss: 0.693462, acc.: 48.44%] [G loss: 0.707012]\n",
            "535 [D loss: 0.690581, acc.: 51.56%] [G loss: 0.714657]\n",
            "536 [D loss: 0.692684, acc.: 53.12%] [G loss: 0.708181]\n",
            "537 [D loss: 0.695190, acc.: 60.94%] [G loss: 0.702554]\n",
            "538 [D loss: 0.698714, acc.: 45.31%] [G loss: 0.703509]\n",
            "539 [D loss: 0.693001, acc.: 45.31%] [G loss: 0.704877]\n",
            "540 [D loss: 0.693810, acc.: 46.88%] [G loss: 0.703170]\n",
            "541 [D loss: 0.691115, acc.: 51.56%] [G loss: 0.705094]\n",
            "542 [D loss: 0.690761, acc.: 53.12%] [G loss: 0.704046]\n",
            "543 [D loss: 0.692913, acc.: 51.56%] [G loss: 0.706635]\n",
            "544 [D loss: 0.695112, acc.: 46.88%] [G loss: 0.704714]\n",
            "545 [D loss: 0.693882, acc.: 40.62%] [G loss: 0.704757]\n",
            "546 [D loss: 0.693704, acc.: 46.88%] [G loss: 0.703562]\n",
            "547 [D loss: 0.692612, acc.: 51.56%] [G loss: 0.709595]\n",
            "548 [D loss: 0.691356, acc.: 48.44%] [G loss: 0.709310]\n",
            "549 [D loss: 0.693504, acc.: 48.44%] [G loss: 0.706963]\n",
            "550 [D loss: 0.693392, acc.: 51.56%] [G loss: 0.709779]\n",
            "551 [D loss: 0.692920, acc.: 42.19%] [G loss: 0.709254]\n",
            "552 [D loss: 0.692927, acc.: 51.56%] [G loss: 0.707981]\n",
            "553 [D loss: 0.696119, acc.: 53.12%] [G loss: 0.707464]\n",
            "554 [D loss: 0.697119, acc.: 48.44%] [G loss: 0.704009]\n",
            "555 [D loss: 0.693700, acc.: 37.50%] [G loss: 0.702770]\n",
            "556 [D loss: 0.693365, acc.: 46.88%] [G loss: 0.702299]\n",
            "557 [D loss: 0.694137, acc.: 46.88%] [G loss: 0.701974]\n",
            "558 [D loss: 0.692617, acc.: 50.00%] [G loss: 0.703361]\n",
            "559 [D loss: 0.691824, acc.: 50.00%] [G loss: 0.701977]\n",
            "560 [D loss: 0.691234, acc.: 48.44%] [G loss: 0.702124]\n",
            "561 [D loss: 0.692349, acc.: 56.25%] [G loss: 0.702928]\n",
            "562 [D loss: 0.692157, acc.: 53.12%] [G loss: 0.701651]\n",
            "563 [D loss: 0.689474, acc.: 50.00%] [G loss: 0.700629]\n",
            "564 [D loss: 0.690838, acc.: 48.44%] [G loss: 0.699553]\n",
            "565 [D loss: 0.697744, acc.: 34.38%] [G loss: 0.700169]\n",
            "566 [D loss: 0.691328, acc.: 54.69%] [G loss: 0.700130]\n",
            "567 [D loss: 0.688518, acc.: 65.62%] [G loss: 0.699062]\n",
            "568 [D loss: 0.690393, acc.: 54.69%] [G loss: 0.698210]\n",
            "569 [D loss: 0.691167, acc.: 50.00%] [G loss: 0.697972]\n",
            "570 [D loss: 0.698319, acc.: 45.31%] [G loss: 0.701037]\n",
            "571 [D loss: 0.695681, acc.: 45.31%] [G loss: 0.698756]\n",
            "572 [D loss: 0.692422, acc.: 54.69%] [G loss: 0.697410]\n",
            "573 [D loss: 0.696561, acc.: 43.75%] [G loss: 0.699579]\n",
            "574 [D loss: 0.690064, acc.: 59.38%] [G loss: 0.698331]\n",
            "575 [D loss: 0.694629, acc.: 48.44%] [G loss: 0.700307]\n",
            "576 [D loss: 0.691507, acc.: 53.12%] [G loss: 0.698398]\n",
            "577 [D loss: 0.695366, acc.: 50.00%] [G loss: 0.698959]\n",
            "578 [D loss: 0.692683, acc.: 56.25%] [G loss: 0.694732]\n",
            "579 [D loss: 0.690907, acc.: 56.25%] [G loss: 0.699835]\n",
            "580 [D loss: 0.692083, acc.: 56.25%] [G loss: 0.699427]\n",
            "581 [D loss: 0.691657, acc.: 60.94%] [G loss: 0.693372]\n",
            "582 [D loss: 0.692732, acc.: 50.00%] [G loss: 0.699597]\n",
            "583 [D loss: 0.689261, acc.: 60.94%] [G loss: 0.700071]\n",
            "584 [D loss: 0.689925, acc.: 59.38%] [G loss: 0.695444]\n",
            "585 [D loss: 0.692388, acc.: 56.25%] [G loss: 0.698133]\n",
            "586 [D loss: 0.691683, acc.: 56.25%] [G loss: 0.698888]\n",
            "587 [D loss: 0.694859, acc.: 46.88%] [G loss: 0.692539]\n",
            "588 [D loss: 0.694789, acc.: 42.19%] [G loss: 0.693413]\n",
            "589 [D loss: 0.690963, acc.: 54.69%] [G loss: 0.697007]\n",
            "590 [D loss: 0.689239, acc.: 60.94%] [G loss: 0.697188]\n",
            "591 [D loss: 0.691847, acc.: 54.69%] [G loss: 0.695367]\n",
            "592 [D loss: 0.698455, acc.: 46.88%] [G loss: 0.698418]\n",
            "593 [D loss: 0.695495, acc.: 42.19%] [G loss: 0.700072]\n",
            "594 [D loss: 0.697123, acc.: 48.44%] [G loss: 0.697842]\n",
            "595 [D loss: 0.695437, acc.: 46.88%] [G loss: 0.698614]\n",
            "596 [D loss: 0.694983, acc.: 50.00%] [G loss: 0.701340]\n",
            "597 [D loss: 0.692627, acc.: 56.25%] [G loss: 0.700994]\n",
            "598 [D loss: 0.695643, acc.: 40.62%] [G loss: 0.703954]\n",
            "599 [D loss: 0.690269, acc.: 59.38%] [G loss: 0.703087]\n",
            "600 [D loss: 0.694897, acc.: 45.31%] [G loss: 0.699603]\n",
            "601 [D loss: 0.692857, acc.: 51.56%] [G loss: 0.705283]\n",
            "602 [D loss: 0.693394, acc.: 48.44%] [G loss: 0.702202]\n",
            "603 [D loss: 0.692603, acc.: 53.12%] [G loss: 0.701544]\n",
            "604 [D loss: 0.694515, acc.: 48.44%] [G loss: 0.700371]\n",
            "605 [D loss: 0.693352, acc.: 50.00%] [G loss: 0.702923]\n",
            "606 [D loss: 0.696659, acc.: 39.06%] [G loss: 0.701862]\n",
            "607 [D loss: 0.693626, acc.: 46.88%] [G loss: 0.696938]\n",
            "608 [D loss: 0.693084, acc.: 53.12%] [G loss: 0.696432]\n",
            "609 [D loss: 0.692980, acc.: 50.00%] [G loss: 0.699878]\n",
            "610 [D loss: 0.690332, acc.: 57.81%] [G loss: 0.700876]\n",
            "611 [D loss: 0.691200, acc.: 51.56%] [G loss: 0.697139]\n",
            "612 [D loss: 0.692544, acc.: 50.00%] [G loss: 0.702743]\n",
            "613 [D loss: 0.687272, acc.: 71.88%] [G loss: 0.700810]\n",
            "614 [D loss: 0.691611, acc.: 45.31%] [G loss: 0.698122]\n",
            "615 [D loss: 0.692079, acc.: 46.88%] [G loss: 0.702832]\n",
            "616 [D loss: 0.693689, acc.: 43.75%] [G loss: 0.695531]\n",
            "617 [D loss: 0.691697, acc.: 53.12%] [G loss: 0.698157]\n",
            "618 [D loss: 0.689537, acc.: 53.12%] [G loss: 0.697119]\n",
            "619 [D loss: 0.688238, acc.: 50.00%] [G loss: 0.691610]\n",
            "620 [D loss: 0.687120, acc.: 62.50%] [G loss: 0.694622]\n",
            "621 [D loss: 0.694708, acc.: 51.56%] [G loss: 0.696388]\n",
            "622 [D loss: 0.696448, acc.: 54.69%] [G loss: 0.698835]\n",
            "623 [D loss: 0.690402, acc.: 46.88%] [G loss: 0.696799]\n",
            "624 [D loss: 0.691923, acc.: 46.88%] [G loss: 0.703769]\n",
            "625 [D loss: 0.692582, acc.: 54.69%] [G loss: 0.709715]\n",
            "626 [D loss: 0.694179, acc.: 54.69%] [G loss: 0.709427]\n",
            "627 [D loss: 0.689368, acc.: 51.56%] [G loss: 0.713280]\n",
            "628 [D loss: 0.694539, acc.: 50.00%] [G loss: 0.710458]\n",
            "629 [D loss: 0.697608, acc.: 43.75%] [G loss: 0.709333]\n",
            "630 [D loss: 0.692125, acc.: 53.12%] [G loss: 0.707538]\n",
            "631 [D loss: 0.692254, acc.: 45.31%] [G loss: 0.708952]\n",
            "632 [D loss: 0.693233, acc.: 59.38%] [G loss: 0.716115]\n",
            "633 [D loss: 0.695729, acc.: 46.88%] [G loss: 0.718285]\n",
            "634 [D loss: 0.692961, acc.: 43.75%] [G loss: 0.708811]\n",
            "635 [D loss: 0.697654, acc.: 46.88%] [G loss: 0.710350]\n",
            "636 [D loss: 0.694494, acc.: 46.88%] [G loss: 0.707909]\n",
            "637 [D loss: 0.698463, acc.: 35.94%] [G loss: 0.705074]\n",
            "638 [D loss: 0.694973, acc.: 40.62%] [G loss: 0.702569]\n",
            "639 [D loss: 0.693282, acc.: 51.56%] [G loss: 0.705353]\n",
            "640 [D loss: 0.696360, acc.: 46.88%] [G loss: 0.701350]\n",
            "641 [D loss: 0.689272, acc.: 53.12%] [G loss: 0.709366]\n",
            "642 [D loss: 0.690590, acc.: 48.44%] [G loss: 0.701874]\n",
            "643 [D loss: 0.690582, acc.: 54.69%] [G loss: 0.707086]\n",
            "644 [D loss: 0.696564, acc.: 46.88%] [G loss: 0.707476]\n",
            "645 [D loss: 0.693408, acc.: 51.56%] [G loss: 0.706129]\n",
            "646 [D loss: 0.694874, acc.: 53.12%] [G loss: 0.707302]\n",
            "647 [D loss: 0.691600, acc.: 54.69%] [G loss: 0.703836]\n",
            "648 [D loss: 0.691470, acc.: 53.12%] [G loss: 0.705622]\n",
            "649 [D loss: 0.695220, acc.: 56.25%] [G loss: 0.704414]\n",
            "650 [D loss: 0.694741, acc.: 48.44%] [G loss: 0.704161]\n",
            "651 [D loss: 0.692236, acc.: 56.25%] [G loss: 0.699649]\n",
            "652 [D loss: 0.690259, acc.: 57.81%] [G loss: 0.701211]\n",
            "653 [D loss: 0.692232, acc.: 54.69%] [G loss: 0.697295]\n",
            "654 [D loss: 0.692196, acc.: 45.31%] [G loss: 0.700604]\n",
            "655 [D loss: 0.688964, acc.: 57.81%] [G loss: 0.702623]\n",
            "656 [D loss: 0.690920, acc.: 59.38%] [G loss: 0.700774]\n",
            "657 [D loss: 0.694485, acc.: 45.31%] [G loss: 0.706319]\n",
            "658 [D loss: 0.694389, acc.: 50.00%] [G loss: 0.702725]\n",
            "659 [D loss: 0.693168, acc.: 53.12%] [G loss: 0.702887]\n",
            "660 [D loss: 0.688810, acc.: 60.94%] [G loss: 0.700334]\n",
            "661 [D loss: 0.699181, acc.: 46.88%] [G loss: 0.702281]\n",
            "662 [D loss: 0.697314, acc.: 43.75%] [G loss: 0.702004]\n",
            "663 [D loss: 0.691495, acc.: 51.56%] [G loss: 0.701717]\n",
            "664 [D loss: 0.693807, acc.: 51.56%] [G loss: 0.699790]\n",
            "665 [D loss: 0.690509, acc.: 60.94%] [G loss: 0.699601]\n",
            "666 [D loss: 0.692386, acc.: 51.56%] [G loss: 0.697864]\n",
            "667 [D loss: 0.696486, acc.: 45.31%] [G loss: 0.699918]\n",
            "668 [D loss: 0.688104, acc.: 65.62%] [G loss: 0.699128]\n",
            "669 [D loss: 0.690195, acc.: 59.38%] [G loss: 0.698064]\n",
            "670 [D loss: 0.693763, acc.: 53.12%] [G loss: 0.692659]\n",
            "671 [D loss: 0.692486, acc.: 46.88%] [G loss: 0.692910]\n",
            "672 [D loss: 0.688209, acc.: 56.25%] [G loss: 0.698295]\n",
            "673 [D loss: 0.691023, acc.: 57.81%] [G loss: 0.689958]\n",
            "674 [D loss: 0.694629, acc.: 43.75%] [G loss: 0.698095]\n",
            "675 [D loss: 0.697038, acc.: 45.31%] [G loss: 0.693632]\n",
            "676 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.693923]\n",
            "677 [D loss: 0.692293, acc.: 53.12%] [G loss: 0.696472]\n",
            "678 [D loss: 0.691264, acc.: 54.69%] [G loss: 0.691036]\n",
            "679 [D loss: 0.693942, acc.: 51.56%] [G loss: 0.693618]\n",
            "680 [D loss: 0.697814, acc.: 42.19%] [G loss: 0.696023]\n",
            "681 [D loss: 0.692666, acc.: 53.12%] [G loss: 0.698462]\n",
            "682 [D loss: 0.694101, acc.: 53.12%] [G loss: 0.699018]\n",
            "683 [D loss: 0.691212, acc.: 62.50%] [G loss: 0.698132]\n",
            "684 [D loss: 0.694334, acc.: 42.19%] [G loss: 0.699192]\n",
            "685 [D loss: 0.692299, acc.: 48.44%] [G loss: 0.703770]\n",
            "686 [D loss: 0.693658, acc.: 48.44%] [G loss: 0.700238]\n",
            "687 [D loss: 0.690790, acc.: 54.69%] [G loss: 0.701849]\n",
            "688 [D loss: 0.695678, acc.: 42.19%] [G loss: 0.702295]\n",
            "689 [D loss: 0.691610, acc.: 50.00%] [G loss: 0.703138]\n",
            "690 [D loss: 0.693070, acc.: 54.69%] [G loss: 0.700775]\n",
            "691 [D loss: 0.693655, acc.: 50.00%] [G loss: 0.700454]\n",
            "692 [D loss: 0.697110, acc.: 48.44%] [G loss: 0.699923]\n",
            "693 [D loss: 0.692355, acc.: 48.44%] [G loss: 0.699508]\n",
            "694 [D loss: 0.687570, acc.: 54.69%] [G loss: 0.703456]\n",
            "695 [D loss: 0.685209, acc.: 62.50%] [G loss: 0.712638]\n",
            "696 [D loss: 0.698164, acc.: 48.44%] [G loss: 0.706352]\n",
            "697 [D loss: 0.691612, acc.: 54.69%] [G loss: 0.705635]\n",
            "698 [D loss: 0.691538, acc.: 57.81%] [G loss: 0.704124]\n",
            "699 [D loss: 0.688565, acc.: 59.38%] [G loss: 0.695189]\n",
            "700 [D loss: 0.693925, acc.: 42.19%] [G loss: 0.694668]\n",
            "701 [D loss: 0.690967, acc.: 53.12%] [G loss: 0.696514]\n",
            "702 [D loss: 0.688400, acc.: 50.00%] [G loss: 0.688496]\n",
            "703 [D loss: 0.696119, acc.: 42.19%] [G loss: 0.690868]\n",
            "704 [D loss: 0.694191, acc.: 42.19%] [G loss: 0.684460]\n",
            "705 [D loss: 0.699403, acc.: 57.81%] [G loss: 0.691568]\n",
            "706 [D loss: 0.695156, acc.: 45.31%] [G loss: 0.688874]\n",
            "707 [D loss: 0.696430, acc.: 42.19%] [G loss: 0.688928]\n",
            "708 [D loss: 0.690756, acc.: 51.56%] [G loss: 0.688017]\n",
            "709 [D loss: 0.692444, acc.: 53.12%] [G loss: 0.687640]\n",
            "710 [D loss: 0.697560, acc.: 42.19%] [G loss: 0.689385]\n",
            "711 [D loss: 0.693357, acc.: 50.00%] [G loss: 0.688515]\n",
            "712 [D loss: 0.692373, acc.: 53.12%] [G loss: 0.690522]\n",
            "713 [D loss: 0.691758, acc.: 51.56%] [G loss: 0.690186]\n",
            "714 [D loss: 0.692573, acc.: 48.44%] [G loss: 0.692468]\n",
            "715 [D loss: 0.691079, acc.: 51.56%] [G loss: 0.688055]\n",
            "716 [D loss: 0.690989, acc.: 51.56%] [G loss: 0.687019]\n",
            "717 [D loss: 0.692998, acc.: 45.31%] [G loss: 0.686225]\n",
            "718 [D loss: 0.691168, acc.: 45.31%] [G loss: 0.690069]\n",
            "719 [D loss: 0.689856, acc.: 48.44%] [G loss: 0.686281]\n",
            "720 [D loss: 0.687502, acc.: 64.06%] [G loss: 0.685536]\n",
            "721 [D loss: 0.690168, acc.: 56.25%] [G loss: 0.690043]\n",
            "722 [D loss: 0.692237, acc.: 45.31%] [G loss: 0.682285]\n",
            "723 [D loss: 0.698581, acc.: 46.88%] [G loss: 0.695865]\n",
            "724 [D loss: 0.692999, acc.: 50.00%] [G loss: 0.689578]\n",
            "725 [D loss: 0.692914, acc.: 45.31%] [G loss: 0.690419]\n",
            "726 [D loss: 0.693499, acc.: 46.88%] [G loss: 0.689998]\n",
            "727 [D loss: 0.687562, acc.: 62.50%] [G loss: 0.690573]\n",
            "728 [D loss: 0.690810, acc.: 53.12%] [G loss: 0.694200]\n",
            "729 [D loss: 0.690027, acc.: 54.69%] [G loss: 0.697498]\n",
            "730 [D loss: 0.691342, acc.: 53.12%] [G loss: 0.690528]\n",
            "731 [D loss: 0.692898, acc.: 48.44%] [G loss: 0.694297]\n",
            "732 [D loss: 0.697014, acc.: 51.56%] [G loss: 0.688563]\n",
            "733 [D loss: 0.702669, acc.: 43.75%] [G loss: 0.687416]\n",
            "734 [D loss: 0.696836, acc.: 43.75%] [G loss: 0.700183]\n",
            "735 [D loss: 0.691340, acc.: 48.44%] [G loss: 0.696403]\n",
            "736 [D loss: 0.695507, acc.: 50.00%] [G loss: 0.700484]\n",
            "737 [D loss: 0.689901, acc.: 54.69%] [G loss: 0.707487]\n",
            "738 [D loss: 0.694582, acc.: 50.00%] [G loss: 0.705635]\n",
            "739 [D loss: 0.696136, acc.: 50.00%] [G loss: 0.708358]\n",
            "740 [D loss: 0.689266, acc.: 57.81%] [G loss: 0.714088]\n",
            "741 [D loss: 0.693267, acc.: 54.69%] [G loss: 0.709201]\n",
            "742 [D loss: 0.690776, acc.: 62.50%] [G loss: 0.716450]\n",
            "743 [D loss: 0.689614, acc.: 57.81%] [G loss: 0.714112]\n",
            "744 [D loss: 0.691100, acc.: 51.56%] [G loss: 0.710484]\n",
            "745 [D loss: 0.691656, acc.: 51.56%] [G loss: 0.711062]\n",
            "746 [D loss: 0.690312, acc.: 56.25%] [G loss: 0.720605]\n",
            "747 [D loss: 0.687532, acc.: 60.94%] [G loss: 0.724817]\n",
            "748 [D loss: 0.696514, acc.: 51.56%] [G loss: 0.714605]\n",
            "749 [D loss: 0.692139, acc.: 43.75%] [G loss: 0.714513]\n",
            "750 [D loss: 0.687235, acc.: 56.25%] [G loss: 0.718941]\n",
            "751 [D loss: 0.689458, acc.: 59.38%] [G loss: 0.718125]\n",
            "752 [D loss: 0.687447, acc.: 64.06%] [G loss: 0.718641]\n",
            "753 [D loss: 0.696050, acc.: 46.88%] [G loss: 0.723649]\n",
            "754 [D loss: 0.691853, acc.: 46.88%] [G loss: 0.722404]\n",
            "755 [D loss: 0.691897, acc.: 54.69%] [G loss: 0.730904]\n",
            "756 [D loss: 0.702609, acc.: 45.31%] [G loss: 0.713642]\n",
            "757 [D loss: 0.690125, acc.: 51.56%] [G loss: 0.715036]\n",
            "758 [D loss: 0.692912, acc.: 40.62%] [G loss: 0.717320]\n",
            "759 [D loss: 0.697460, acc.: 48.44%] [G loss: 0.718558]\n",
            "760 [D loss: 0.700367, acc.: 40.62%] [G loss: 0.709607]\n",
            "761 [D loss: 0.694253, acc.: 45.31%] [G loss: 0.706145]\n",
            "762 [D loss: 0.696530, acc.: 51.56%] [G loss: 0.708004]\n",
            "763 [D loss: 0.694100, acc.: 48.44%] [G loss: 0.700702]\n",
            "764 [D loss: 0.694806, acc.: 53.12%] [G loss: 0.704956]\n",
            "765 [D loss: 0.694302, acc.: 45.31%] [G loss: 0.706143]\n",
            "766 [D loss: 0.691561, acc.: 54.69%] [G loss: 0.700707]\n",
            "767 [D loss: 0.694821, acc.: 46.88%] [G loss: 0.709813]\n",
            "768 [D loss: 0.692206, acc.: 60.94%] [G loss: 0.707542]\n",
            "769 [D loss: 0.694523, acc.: 42.19%] [G loss: 0.703766]\n",
            "770 [D loss: 0.689932, acc.: 56.25%] [G loss: 0.704149]\n",
            "771 [D loss: 0.691841, acc.: 53.12%] [G loss: 0.703902]\n",
            "772 [D loss: 0.693790, acc.: 48.44%] [G loss: 0.701401]\n",
            "773 [D loss: 0.696839, acc.: 42.19%] [G loss: 0.704984]\n",
            "774 [D loss: 0.693415, acc.: 48.44%] [G loss: 0.702054]\n",
            "775 [D loss: 0.694670, acc.: 45.31%] [G loss: 0.702135]\n",
            "776 [D loss: 0.690955, acc.: 53.12%] [G loss: 0.699036]\n",
            "777 [D loss: 0.692472, acc.: 54.69%] [G loss: 0.701301]\n",
            "778 [D loss: 0.692326, acc.: 53.12%] [G loss: 0.701650]\n",
            "779 [D loss: 0.690040, acc.: 57.81%] [G loss: 0.707363]\n",
            "780 [D loss: 0.698484, acc.: 42.19%] [G loss: 0.701567]\n",
            "781 [D loss: 0.695631, acc.: 42.19%] [G loss: 0.702626]\n",
            "782 [D loss: 0.693211, acc.: 45.31%] [G loss: 0.701688]\n",
            "783 [D loss: 0.692496, acc.: 50.00%] [G loss: 0.702177]\n",
            "784 [D loss: 0.694035, acc.: 51.56%] [G loss: 0.700304]\n",
            "785 [D loss: 0.695479, acc.: 43.75%] [G loss: 0.694357]\n",
            "786 [D loss: 0.693252, acc.: 51.56%] [G loss: 0.698853]\n",
            "787 [D loss: 0.691827, acc.: 59.38%] [G loss: 0.698670]\n",
            "788 [D loss: 0.693438, acc.: 46.88%] [G loss: 0.696425]\n",
            "789 [D loss: 0.693799, acc.: 54.69%] [G loss: 0.694965]\n",
            "790 [D loss: 0.691626, acc.: 53.12%] [G loss: 0.696589]\n",
            "791 [D loss: 0.691089, acc.: 59.38%] [G loss: 0.694056]\n",
            "792 [D loss: 0.698673, acc.: 37.50%] [G loss: 0.695379]\n",
            "793 [D loss: 0.693046, acc.: 56.25%] [G loss: 0.697662]\n",
            "794 [D loss: 0.694777, acc.: 40.62%] [G loss: 0.695794]\n",
            "795 [D loss: 0.692280, acc.: 50.00%] [G loss: 0.696954]\n",
            "796 [D loss: 0.692517, acc.: 48.44%] [G loss: 0.698928]\n",
            "797 [D loss: 0.692212, acc.: 53.12%] [G loss: 0.697147]\n",
            "798 [D loss: 0.692797, acc.: 56.25%] [G loss: 0.696905]\n",
            "799 [D loss: 0.691999, acc.: 53.12%] [G loss: 0.703066]\n",
            "800 [D loss: 0.692090, acc.: 45.31%] [G loss: 0.701143]\n",
            "801 [D loss: 0.689212, acc.: 62.50%] [G loss: 0.702753]\n",
            "802 [D loss: 0.690422, acc.: 62.50%] [G loss: 0.701094]\n",
            "803 [D loss: 0.691578, acc.: 53.12%] [G loss: 0.699766]\n",
            "804 [D loss: 0.692931, acc.: 51.56%] [G loss: 0.701301]\n",
            "805 [D loss: 0.694069, acc.: 60.94%] [G loss: 0.700522]\n",
            "806 [D loss: 0.692810, acc.: 53.12%] [G loss: 0.699728]\n",
            "807 [D loss: 0.690724, acc.: 50.00%] [G loss: 0.700320]\n",
            "808 [D loss: 0.694148, acc.: 40.62%] [G loss: 0.697226]\n",
            "809 [D loss: 0.691878, acc.: 53.12%] [G loss: 0.698214]\n",
            "810 [D loss: 0.693940, acc.: 48.44%] [G loss: 0.696407]\n",
            "811 [D loss: 0.694669, acc.: 50.00%] [G loss: 0.700926]\n",
            "812 [D loss: 0.693838, acc.: 50.00%] [G loss: 0.697510]\n",
            "813 [D loss: 0.691276, acc.: 57.81%] [G loss: 0.698559]\n",
            "814 [D loss: 0.694348, acc.: 57.81%] [G loss: 0.701766]\n",
            "815 [D loss: 0.694920, acc.: 51.56%] [G loss: 0.692772]\n",
            "816 [D loss: 0.692726, acc.: 48.44%] [G loss: 0.695785]\n",
            "817 [D loss: 0.692813, acc.: 50.00%] [G loss: 0.696248]\n",
            "818 [D loss: 0.691640, acc.: 54.69%] [G loss: 0.691631]\n",
            "819 [D loss: 0.694752, acc.: 46.88%] [G loss: 0.693029]\n",
            "820 [D loss: 0.695268, acc.: 43.75%] [G loss: 0.692370]\n",
            "821 [D loss: 0.688824, acc.: 65.62%] [G loss: 0.687622]\n",
            "822 [D loss: 0.691622, acc.: 53.12%] [G loss: 0.689770]\n",
            "823 [D loss: 0.694156, acc.: 45.31%] [G loss: 0.694658]\n",
            "824 [D loss: 0.690841, acc.: 53.12%] [G loss: 0.694952]\n",
            "825 [D loss: 0.690370, acc.: 56.25%] [G loss: 0.695648]\n",
            "826 [D loss: 0.691441, acc.: 54.69%] [G loss: 0.689740]\n",
            "827 [D loss: 0.692164, acc.: 53.12%] [G loss: 0.690538]\n",
            "828 [D loss: 0.692180, acc.: 51.56%] [G loss: 0.689962]\n",
            "829 [D loss: 0.694905, acc.: 40.62%] [G loss: 0.690770]\n",
            "830 [D loss: 0.691194, acc.: 48.44%] [G loss: 0.695599]\n",
            "831 [D loss: 0.693147, acc.: 37.50%] [G loss: 0.691463]\n",
            "832 [D loss: 0.695504, acc.: 45.31%] [G loss: 0.693351]\n",
            "833 [D loss: 0.688376, acc.: 65.62%] [G loss: 0.692807]\n",
            "834 [D loss: 0.692787, acc.: 57.81%] [G loss: 0.695446]\n",
            "835 [D loss: 0.689951, acc.: 56.25%] [G loss: 0.694998]\n",
            "836 [D loss: 0.691183, acc.: 59.38%] [G loss: 0.692685]\n",
            "837 [D loss: 0.695304, acc.: 43.75%] [G loss: 0.691957]\n",
            "838 [D loss: 0.687660, acc.: 59.38%] [G loss: 0.690152]\n",
            "839 [D loss: 0.690582, acc.: 51.56%] [G loss: 0.695598]\n",
            "840 [D loss: 0.686270, acc.: 60.94%] [G loss: 0.692204]\n",
            "841 [D loss: 0.694745, acc.: 39.06%] [G loss: 0.696846]\n",
            "842 [D loss: 0.688995, acc.: 54.69%] [G loss: 0.696185]\n",
            "843 [D loss: 0.691187, acc.: 51.56%] [G loss: 0.694143]\n",
            "844 [D loss: 0.693516, acc.: 43.75%] [G loss: 0.701659]\n",
            "845 [D loss: 0.689286, acc.: 51.56%] [G loss: 0.707107]\n",
            "846 [D loss: 0.691402, acc.: 56.25%] [G loss: 0.709849]\n",
            "847 [D loss: 0.694142, acc.: 50.00%] [G loss: 0.703888]\n",
            "848 [D loss: 0.696191, acc.: 46.88%] [G loss: 0.702899]\n",
            "849 [D loss: 0.691869, acc.: 51.56%] [G loss: 0.702155]\n",
            "850 [D loss: 0.697267, acc.: 42.19%] [G loss: 0.704064]\n",
            "851 [D loss: 0.692508, acc.: 53.12%] [G loss: 0.708508]\n",
            "852 [D loss: 0.690519, acc.: 56.25%] [G loss: 0.698368]\n",
            "853 [D loss: 0.691365, acc.: 56.25%] [G loss: 0.697186]\n",
            "854 [D loss: 0.691785, acc.: 53.12%] [G loss: 0.698344]\n",
            "855 [D loss: 0.694733, acc.: 51.56%] [G loss: 0.702802]\n",
            "856 [D loss: 0.697031, acc.: 48.44%] [G loss: 0.702992]\n",
            "857 [D loss: 0.693798, acc.: 48.44%] [G loss: 0.701460]\n",
            "858 [D loss: 0.692854, acc.: 53.12%] [G loss: 0.701631]\n",
            "859 [D loss: 0.690669, acc.: 57.81%] [G loss: 0.704747]\n",
            "860 [D loss: 0.692562, acc.: 46.88%] [G loss: 0.701079]\n",
            "861 [D loss: 0.689817, acc.: 53.12%] [G loss: 0.703177]\n",
            "862 [D loss: 0.688030, acc.: 54.69%] [G loss: 0.700417]\n",
            "863 [D loss: 0.685750, acc.: 64.06%] [G loss: 0.701345]\n",
            "864 [D loss: 0.682343, acc.: 67.19%] [G loss: 0.699640]\n",
            "865 [D loss: 0.690946, acc.: 53.12%] [G loss: 0.700775]\n",
            "866 [D loss: 0.693736, acc.: 48.44%] [G loss: 0.698289]\n",
            "867 [D loss: 0.694799, acc.: 50.00%] [G loss: 0.701019]\n",
            "868 [D loss: 0.690298, acc.: 53.12%] [G loss: 0.700432]\n",
            "869 [D loss: 0.692102, acc.: 51.56%] [G loss: 0.707507]\n",
            "870 [D loss: 0.696458, acc.: 43.75%] [G loss: 0.701160]\n",
            "871 [D loss: 0.692752, acc.: 51.56%] [G loss: 0.701483]\n",
            "872 [D loss: 0.692050, acc.: 54.69%] [G loss: 0.708637]\n",
            "873 [D loss: 0.693597, acc.: 51.56%] [G loss: 0.705435]\n",
            "874 [D loss: 0.692029, acc.: 53.12%] [G loss: 0.713329]\n",
            "875 [D loss: 0.692460, acc.: 45.31%] [G loss: 0.706509]\n",
            "876 [D loss: 0.697309, acc.: 42.19%] [G loss: 0.720406]\n",
            "877 [D loss: 0.697758, acc.: 48.44%] [G loss: 0.714933]\n",
            "878 [D loss: 0.689328, acc.: 50.00%] [G loss: 0.712122]\n",
            "879 [D loss: 0.694041, acc.: 51.56%] [G loss: 0.720131]\n",
            "880 [D loss: 0.692548, acc.: 48.44%] [G loss: 0.709749]\n",
            "881 [D loss: 0.695843, acc.: 53.12%] [G loss: 0.709672]\n",
            "882 [D loss: 0.694219, acc.: 48.44%] [G loss: 0.702312]\n",
            "883 [D loss: 0.695650, acc.: 53.12%] [G loss: 0.700363]\n",
            "884 [D loss: 0.693702, acc.: 53.12%] [G loss: 0.707009]\n",
            "885 [D loss: 0.695214, acc.: 45.31%] [G loss: 0.700974]\n",
            "886 [D loss: 0.690175, acc.: 64.06%] [G loss: 0.700031]\n",
            "887 [D loss: 0.691764, acc.: 50.00%] [G loss: 0.701319]\n",
            "888 [D loss: 0.690357, acc.: 67.19%] [G loss: 0.695604]\n",
            "889 [D loss: 0.691809, acc.: 50.00%] [G loss: 0.698938]\n",
            "890 [D loss: 0.695449, acc.: 48.44%] [G loss: 0.689048]\n",
            "891 [D loss: 0.694950, acc.: 43.75%] [G loss: 0.693519]\n",
            "892 [D loss: 0.691768, acc.: 53.12%] [G loss: 0.694601]\n",
            "893 [D loss: 0.693729, acc.: 51.56%] [G loss: 0.691428]\n",
            "894 [D loss: 0.691936, acc.: 60.94%] [G loss: 0.691483]\n",
            "895 [D loss: 0.693244, acc.: 51.56%] [G loss: 0.691688]\n",
            "896 [D loss: 0.690661, acc.: 57.81%] [G loss: 0.690127]\n",
            "897 [D loss: 0.691713, acc.: 57.81%] [G loss: 0.683991]\n",
            "898 [D loss: 0.691743, acc.: 57.81%] [G loss: 0.690449]\n",
            "899 [D loss: 0.690979, acc.: 59.38%] [G loss: 0.692866]\n",
            "900 [D loss: 0.692526, acc.: 43.75%] [G loss: 0.688678]\n",
            "901 [D loss: 0.691312, acc.: 56.25%] [G loss: 0.690299]\n",
            "902 [D loss: 0.695445, acc.: 42.19%] [G loss: 0.692988]\n",
            "903 [D loss: 0.688546, acc.: 53.12%] [G loss: 0.694877]\n",
            "904 [D loss: 0.693436, acc.: 53.12%] [G loss: 0.694577]\n",
            "905 [D loss: 0.692805, acc.: 46.88%] [G loss: 0.700202]\n",
            "906 [D loss: 0.693285, acc.: 46.88%] [G loss: 0.698096]\n",
            "907 [D loss: 0.696644, acc.: 46.88%] [G loss: 0.693317]\n",
            "908 [D loss: 0.696311, acc.: 48.44%] [G loss: 0.693303]\n",
            "909 [D loss: 0.691414, acc.: 56.25%] [G loss: 0.695833]\n",
            "910 [D loss: 0.694562, acc.: 45.31%] [G loss: 0.696548]\n",
            "911 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.700004]\n",
            "912 [D loss: 0.690075, acc.: 56.25%] [G loss: 0.696274]\n",
            "913 [D loss: 0.695210, acc.: 50.00%] [G loss: 0.697473]\n",
            "914 [D loss: 0.696436, acc.: 45.31%] [G loss: 0.699826]\n",
            "915 [D loss: 0.692056, acc.: 45.31%] [G loss: 0.698690]\n",
            "916 [D loss: 0.696652, acc.: 35.94%] [G loss: 0.699919]\n",
            "917 [D loss: 0.690177, acc.: 53.12%] [G loss: 0.700916]\n",
            "918 [D loss: 0.691500, acc.: 56.25%] [G loss: 0.702966]\n",
            "919 [D loss: 0.692935, acc.: 54.69%] [G loss: 0.699753]\n",
            "920 [D loss: 0.689372, acc.: 68.75%] [G loss: 0.702462]\n",
            "921 [D loss: 0.694581, acc.: 50.00%] [G loss: 0.697225]\n",
            "922 [D loss: 0.688705, acc.: 60.94%] [G loss: 0.702667]\n",
            "923 [D loss: 0.690010, acc.: 50.00%] [G loss: 0.708046]\n",
            "924 [D loss: 0.692358, acc.: 50.00%] [G loss: 0.703041]\n",
            "925 [D loss: 0.692859, acc.: 56.25%] [G loss: 0.706056]\n",
            "926 [D loss: 0.683463, acc.: 62.50%] [G loss: 0.705304]\n",
            "927 [D loss: 0.690831, acc.: 53.12%] [G loss: 0.707046]\n",
            "928 [D loss: 0.694946, acc.: 46.88%] [G loss: 0.707813]\n",
            "929 [D loss: 0.686459, acc.: 57.81%] [G loss: 0.707702]\n",
            "930 [D loss: 0.694453, acc.: 46.88%] [G loss: 0.709887]\n",
            "931 [D loss: 0.694010, acc.: 51.56%] [G loss: 0.704934]\n",
            "932 [D loss: 0.691176, acc.: 48.44%] [G loss: 0.701841]\n",
            "933 [D loss: 0.689166, acc.: 59.38%] [G loss: 0.705667]\n",
            "934 [D loss: 0.688253, acc.: 54.69%] [G loss: 0.717348]\n",
            "935 [D loss: 0.699217, acc.: 46.88%] [G loss: 0.710654]\n",
            "936 [D loss: 0.689229, acc.: 54.69%] [G loss: 0.708607]\n",
            "937 [D loss: 0.689043, acc.: 51.56%] [G loss: 0.707746]\n",
            "938 [D loss: 0.689354, acc.: 53.12%] [G loss: 0.707304]\n",
            "939 [D loss: 0.690402, acc.: 59.38%] [G loss: 0.721618]\n",
            "940 [D loss: 0.696445, acc.: 53.12%] [G loss: 0.709986]\n",
            "941 [D loss: 0.703212, acc.: 37.50%] [G loss: 0.699017]\n",
            "942 [D loss: 0.690570, acc.: 56.25%] [G loss: 0.702589]\n",
            "943 [D loss: 0.691610, acc.: 48.44%] [G loss: 0.709880]\n",
            "944 [D loss: 0.695137, acc.: 48.44%] [G loss: 0.703045]\n",
            "945 [D loss: 0.685579, acc.: 56.25%] [G loss: 0.707429]\n",
            "946 [D loss: 0.691480, acc.: 48.44%] [G loss: 0.700682]\n",
            "947 [D loss: 0.699880, acc.: 42.19%] [G loss: 0.699766]\n",
            "948 [D loss: 0.690253, acc.: 51.56%] [G loss: 0.699848]\n",
            "949 [D loss: 0.693880, acc.: 54.69%] [G loss: 0.696923]\n",
            "950 [D loss: 0.689347, acc.: 57.81%] [G loss: 0.706030]\n",
            "951 [D loss: 0.689931, acc.: 53.12%] [G loss: 0.699649]\n",
            "952 [D loss: 0.692652, acc.: 51.56%] [G loss: 0.705323]\n",
            "953 [D loss: 0.691633, acc.: 48.44%] [G loss: 0.702430]\n",
            "954 [D loss: 0.690590, acc.: 50.00%] [G loss: 0.705162]\n",
            "955 [D loss: 0.693887, acc.: 42.19%] [G loss: 0.711439]\n",
            "956 [D loss: 0.690008, acc.: 54.69%] [G loss: 0.716479]\n",
            "957 [D loss: 0.694881, acc.: 39.06%] [G loss: 0.716128]\n",
            "958 [D loss: 0.694676, acc.: 46.88%] [G loss: 0.711222]\n",
            "959 [D loss: 0.690894, acc.: 59.38%] [G loss: 0.710651]\n",
            "960 [D loss: 0.692721, acc.: 48.44%] [G loss: 0.714257]\n",
            "961 [D loss: 0.691611, acc.: 46.88%] [G loss: 0.709864]\n",
            "962 [D loss: 0.690106, acc.: 53.12%] [G loss: 0.713854]\n",
            "963 [D loss: 0.688080, acc.: 57.81%] [G loss: 0.720013]\n",
            "964 [D loss: 0.695180, acc.: 51.56%] [G loss: 0.715313]\n",
            "965 [D loss: 0.698517, acc.: 48.44%] [G loss: 0.711021]\n",
            "966 [D loss: 0.696119, acc.: 51.56%] [G loss: 0.714674]\n",
            "967 [D loss: 0.695006, acc.: 45.31%] [G loss: 0.709333]\n",
            "968 [D loss: 0.694533, acc.: 48.44%] [G loss: 0.710524]\n",
            "969 [D loss: 0.700993, acc.: 39.06%] [G loss: 0.702197]\n",
            "970 [D loss: 0.696249, acc.: 46.88%] [G loss: 0.696856]\n",
            "971 [D loss: 0.689840, acc.: 59.38%] [G loss: 0.696369]\n",
            "972 [D loss: 0.690694, acc.: 57.81%] [G loss: 0.700006]\n",
            "973 [D loss: 0.692463, acc.: 46.88%] [G loss: 0.705430]\n",
            "974 [D loss: 0.689928, acc.: 57.81%] [G loss: 0.696347]\n",
            "975 [D loss: 0.693808, acc.: 48.44%] [G loss: 0.703286]\n",
            "976 [D loss: 0.693833, acc.: 46.88%] [G loss: 0.702390]\n",
            "977 [D loss: 0.689652, acc.: 51.56%] [G loss: 0.701795]\n",
            "978 [D loss: 0.696419, acc.: 53.12%] [G loss: 0.699453]\n",
            "979 [D loss: 0.695137, acc.: 42.19%] [G loss: 0.705135]\n",
            "980 [D loss: 0.692686, acc.: 46.88%] [G loss: 0.700059]\n",
            "981 [D loss: 0.696441, acc.: 32.81%] [G loss: 0.704691]\n",
            "982 [D loss: 0.691501, acc.: 56.25%] [G loss: 0.707478]\n",
            "983 [D loss: 0.695769, acc.: 45.31%] [G loss: 0.704606]\n",
            "984 [D loss: 0.699695, acc.: 39.06%] [G loss: 0.700736]\n",
            "985 [D loss: 0.689930, acc.: 51.56%] [G loss: 0.699253]\n",
            "986 [D loss: 0.695017, acc.: 46.88%] [G loss: 0.699795]\n",
            "987 [D loss: 0.693487, acc.: 51.56%] [G loss: 0.697622]\n",
            "988 [D loss: 0.692839, acc.: 50.00%] [G loss: 0.700781]\n",
            "989 [D loss: 0.694982, acc.: 45.31%] [G loss: 0.700064]\n",
            "990 [D loss: 0.691409, acc.: 54.69%] [G loss: 0.701067]\n",
            "991 [D loss: 0.690231, acc.: 54.69%] [G loss: 0.703066]\n",
            "992 [D loss: 0.689699, acc.: 56.25%] [G loss: 0.698888]\n",
            "993 [D loss: 0.693920, acc.: 50.00%] [G loss: 0.701558]\n",
            "994 [D loss: 0.689929, acc.: 53.12%] [G loss: 0.715883]\n",
            "995 [D loss: 0.696840, acc.: 39.06%] [G loss: 0.709987]\n",
            "996 [D loss: 0.690996, acc.: 54.69%] [G loss: 0.709951]\n",
            "997 [D loss: 0.694044, acc.: 54.69%] [G loss: 0.696245]\n",
            "998 [D loss: 0.690831, acc.: 53.12%] [G loss: 0.701824]\n",
            "999 [D loss: 0.688829, acc.: 59.38%] [G loss: 0.703892]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBiKOdy8PJx6",
        "colab_type": "code",
        "outputId": "a51bea47-5bc8-478d-edec-ae4ea57bf0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRlEJWuul3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}